{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"timyuan.devs","text":"<p>Hey! I'm Tim Yuan. \ud83d\udc4b</p> <p>Welcome to my personal website and portfolio.</p> <p>I'm a computer science student passionate about AI and web development. \ud83c\udf93</p> <p>Currently pursuing my Master's at UIUC, I previously worked as a research intern at UC Irvine on  Texera and CellUniverse. \ud83d\udd2c</p> <p>I'm currently working on an AI supported email client called MailMind. Check this out \ud83d\ude80</p>"},{"location":"learn/","title":"Learning Journey \ud83d\udcda","text":"<p>Welcome to my daily learning log! Here I document my journey through various topics in AI, web development, and computer science.</p>"},{"location":"learn/#current-focus-areas","title":"\ud83c\udfaf Current Focus Areas","text":"<ul> <li>Machine Learning &amp; AI - Deep learning, NLP, computer vision</li> <li>Web Development - React, Node.js, modern frameworks</li> <li>Systems &amp; Architecture - Distributed systems, cloud computing</li> <li>Research Projects - Academic papers and implementations</li> </ul>"},{"location":"learn/#recent-learning-logs","title":"\ud83d\udcc5 Recent Learning Logs","text":""},{"location":"learn/#july-2025","title":"July 2025","text":"<ul> <li>July 3, 2025 - Month Plan setup</li> <li>July 4, 2025 - Leetcode drill</li> <li>July 5, 2025 - Leetcode drill</li> </ul> <p>\"You Don't Procrastinate Because You Lack Motivation, But Because You Lack Clarity.\"</p>"},{"location":"learn/2025/07-july/","title":"Plan for July","text":""},{"location":"learn/2025/07-july/#main-goals","title":"\ud83c\udfaf Main Goals:","text":"<ul> <li> Fall Recruitment Preparation</li> <li> Improve as an AI SDE</li> <li> Hone Spoken English</li> </ul>"},{"location":"learn/2025/07-july/#plan-details","title":"\ud83d\udcc3 Plan Details:","text":"<p>As they say clarity is the biggest enemy against procrastination. So here are a more detailed version of my plans related to each topic.</p>"},{"location":"learn/2025/07-july/#fall-recruitment-preparation","title":"Fall Recruitment Preparation","text":"<p>In general, SDE candidates are tested from the following two aspects:</p> <ul> <li>Techinical </li> <li>Behavior</li> </ul> <p>For Technical Questions it basically falls into these categories:</p> <ul> <li>Leetcode questions</li> <li>Computer Science Basics</li> <li>System Design</li> </ul> <p>So here is how they are gonna fit into my daily schedule:</p> <ul> <li>Complete two Leetcode Problems and redo the previous two (Focus on the real OA problems, and during the redo explain the thinking process, add more problems if we can finish all tasks in time)</li> <li>Find and solve real interview questions related to computer science basics. Review if the concept gets rusty</li> <li>For introduction finish the system design interview book</li> <li>Further study: Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems </li> </ul> <p>For Behavior Questions:</p> <ul> <li>Master STAR methods</li> <li>Practice one real BQ question and review one on a daily basis</li> </ul>"},{"location":"learn/2025/07-july/#improve-as-an-ai-sde","title":"Improve as an AI SDE","text":"<ul> <li>Finish AI Engeering Book </li> <li>Learn MLOp and DevOp tools like: docker, github actions. </li> <li>Master OOP, Python and related Web tech stack</li> <li>Work on personal project (Improve Mailmind )</li> </ul>"},{"location":"learn/2025/07-july/#hone-spoken-english","title":"Hone Spoken English","text":"<ul> <li>Learn and review 5 common phrases </li> <li>Dialogue translation</li> </ul>"},{"location":"learn/2025/07-july/#apply-apply-apply","title":"Apply! Apply! Apply!","text":"<ul> <li>Submit at least one application each day</li> </ul>"},{"location":"learn/2025/07-july/2025-07-03/","title":"July 3, 2025 - Month Plan setup","text":""},{"location":"learn/2025/07-july/2025-07-03/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Create the montly goal</li> <li> First Chapter of AI Engineering Book</li> </ul>"},{"location":"learn/2025/07-july/2025-07-03/#what-i-learned","title":"\ud83d\udcdd What I Learned","text":""},{"location":"learn/2025/07-july/2025-07-03/#overview-of-ai-engineering-chapter-1","title":"Overview of AI Engineering Chapter 1","text":"<p>The first chatper focuses on two aspects: why AI engineering is emerging as a new practice and what are the common practice and processes of developing AI application.</p>"},{"location":"learn/2025/07-july/2025-07-03/#emergence-of-ai-engineering","title":"Emergence of AI Engineering:","text":"<p>After 2023, the development of AI related applications has significantly increased. There are two main driving factors, one is that AI models has scaled up to meet the business needs. Secondly, large companies started their the model as service business. People can now easily access these foundation models through API calls. (You have to pay for it though. The good news is that the price is getting lower and lower) </p>"},{"location":"learn/2025/07-july/2025-07-03/#different-types-of-llm-models","title":"Different Types of LLM models","text":"<p>There are two types of LLM models: Masked language model and Autoregressive language model. To explain in simple terms, masked language model predict missing token using the context form both before and after the missing token like \"My favorite __ is blue\". One of the use case is code debugging. For the most common AIGC application, we gonna choose autoregressive language model which only predicts token based on the previous tokens. After the generation it would takes the generated token into the context. That's where the autoregressive comes from.</p>"},{"location":"learn/2025/07-july/2025-07-03/#token","title":"Token","text":"<p>Token is the smallest unit of vocabulary inside a language model. It could be a word or a part of word like \"ing\". It stands for the smallest meaningful component of a word. People use token instead of word to train LLM for the following reasons</p> <ul> <li>Token stands for smallest meaningful component of a word which can lead to better accuracy during prediction</li> <li>Using token we can actually let model to understand more with limited vocabulary. e.g. to teach the model these words: \"cook\", \"cooking\", \"learn\" and \"learning\" would require 4 entries if we are using the whole word to encode it. However, if we break them down into token, it only requires three tokens \"cook\", \"learn\" and \"ing\"</li> <li>This would help model understand made up words like \"chatgpting\"</li> </ul> <p>Actually, the concept LLM, is already obsolete. The input for models moves beyond text. Companies have already incorporated more data modalities like video and audios to make them more powerful. These models are better characterized as foundation models</p> <p>Self-supervised learning</p> <p>Self-supervised learning is a type of machine learning where the model generates its own label from the raw data so we don't have to manually labeled the data. For LLM, it is obvious that, we can just remove some of the words from the sentence and let it to predict the missing word. It is self-supervised learning that makes the training of LLM economically possible.</p>"},{"location":"learn/2025/07-july/2025-07-03/#layers-of-ai-stack","title":"Layers of AI Stack","text":"<p>Here is a very good illustration of the three lay of current AI stack. </p>"},{"location":"learn/2025/07-july/2025-07-03/#concepts-that-requires-further-study","title":"\ud83d\ude80 Concepts that Requires Further Study:","text":"<ul> <li>Bert</li> <li>Transformer</li> <li>MoE</li> <li>Self-supervised Learning</li> <li>Embedding</li> <li>Goldman Sachs Research</li> <li>FactSet</li> <li>SEO</li> <li>ChatGPT Memory</li> <li>Calendly</li> <li>MailChimp</li> <li>Photoroom</li> <li>TTFT, TPOT, total latency</li> <li>GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models</li> <li>CoT@32</li> </ul>"},{"location":"learn/2025/07-july/2025-07-04/","title":"July 4, 2025 - Leetcode drill","text":""},{"location":"learn/2025/07-july/2025-07-04/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity Training</li> <li> Leetcode problem</li> </ul>"},{"location":"learn/2025/07-july/2025-07-04/#what-i-learned","title":"\ud83d\udcdd What I Learned","text":"<p>3434.Maximum Frequency After Subarray Operation</p>"},{"location":"learn/2025/07-july/2025-07-04/#topics-array-greedy-dynamic-programming-prefix-sum","title":"Topics: Array, Greedy, Dynamic Programming, Prefix Sum","text":""},{"location":"learn/2025/07-july/2025-07-04/#thinking-process","title":"\ud83d\udcdd Thinking Process:","text":"<p>The naive solution would be iterating through all of the possible enumeration of i, j and count the maximum frequency of a particular element within the subarray and then add it to the counts of k before and after the subarray. It would take O(n<sup>3</sup>) time if we don't do any optimization. For optimization we can do a prefix sum to store the frequencies, making it possible to achieve O(1) for frequency range query. Even with optimization the time complexity is still O(n<sup>2</sup>) which is obviously not good enough. One of the common mindset I would love to use when dealing with array is to think of if we know the solution to part of the array how would that help us if a new element is added to that array. (Which is essentially DP) From the prompt, we can see some resemblence to the maximum subarray problem which reduces the O(n<sup>2</sup>) brute force solution to O(n) using Kadane Algorithm</p> <p>Kadane Algorithm</p> <p>Kadane Algorihtm is basically saying one thing: let's iterate through each element of the array and find the maximum subarray that ends with each one of them. The beautiful thing is that if we know the maximum subarray of the previous element, the maximum subarray sum of the current element would either be the sum of this element itself or the element appended to the previous maximum subarray. </p> <p>Here it is slightly trickier than the original Kadane Algorithm. Instead of finding the maximum ending sum of each element, we needs to find the maximum frequency of k if the subarray ends with an element. Since we can mutate any element in the subarray, we might as well fix the mutated element to a particular one to simplify it. (This makes sense since the value range of each element is [1, 50]) After, we fixed the mutated element and calculated the maximum fequency of previous subarray, we can infer the one for the current element.</p> <p>There are two cases:</p> <ul> <li>if the current element is the mutated element. The maximum frequency would always be the previous one + 1</li> <li>if the current element is something else. We would need to check between the result of appending the current element to the previous subarray or using itself as a new subarray.</li> </ul> <p>Here is my initial attempt at the question:</p> <pre><code>class Solution:\n    def getPreCounts(self, prefix_counts, i):\n        return prefix_counts[i]\n\n    def getPostCounts(self, prefix_counts, j):\n        return prefix_counts[-1] - prefix_counts[j + 1]\n\n    def maxFixedFrequency(self, nums, prefix_counts, d, k):\n        cur_freq = max_freq = 0\n        for j in range(len(nums)):\n            post_counts = self.getPostCounts(prefix_counts, j)\n            if nums[j] == d:\n                cur_freq = max(cur_freq + 1, 1 + self.getPreCounts(prefix_counts, j))\n            else:\n                cur_freq = max(cur_freq, self.getPreCounts(prefix_counts, j))\n            max_freq = max(max_freq, cur_freq + post_counts)\n        return max_freq\n\n    def maxFrequency(self, nums: List[int], k: int) -&gt; int:\n        from collections import defaultdict\n        max_freq = 0\n        prefix_counts = [0 for _ in range(len(nums) + 1)]\n        for i in range(len(nums)):\n            prefix_counts[i + 1] = prefix_counts[i]\n            if nums[i] == k:\n                prefix_counts[i + 1] += 1\n        # fix the mutating element\n        for d in range(1, 51):\n            if d == k:\n                max_freq = max(max_freq, prefix_counts[-1])\n                continue\n            max_freq = max(max_freq, self.maxFixedFrequency(nums, prefix_counts, d, k))\n        return max_freq\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-04/#leetcode-and-problem-solving-takeaways","title":"\ud83d\ude80Leetcode and Problem Solving takeaways:","text":"<ul> <li>Using DP mindset to improve time complexity</li> <li>When you see ranges or subarrays, prefix sum would help you perform faster queries</li> <li>Always add new functions for better code readability and modularity. It can also help you think faster. (Think of the function as a black box to speed up your thinking process)</li> </ul>"},{"location":"learn/2025/07-july/2025-07-04/#hiccups-during-implementation","title":"\ud83e\udd26\u200d\u2642\ufe0fHiccups during Implementation:","text":"<ul> <li>prefix sum requires <code>n + 1</code> element to properly represent all range queries. The ith element of the prefix sum means the sum of all element up to <code>i - 1</code> in the original array</li> <li>Subarray is always non empty</li> <li>Understanding of maximum in Kadane Algorithm. Think of why the maximum subarray sum of that ends with the current element would either be the sum of this element itself or the element appended to the previous maximum subarray. </li> <li>Breaking the problem into easier ones. (Fixing the mutating element. Assuming we know the prefix maximum frequency)</li> <li>Classification. What happens when the current element is <code>k</code>? What if it is the mutated element <code>d</code>? What if <code>d</code> and <code>k</code> is the same?</li> </ul>"},{"location":"learn/2025/07-july/2025-07-04/#useful-links","title":"\ud83d\udd17Useful links:","text":"<ul> <li>Maximum Subarray Sum</li> <li>Kadane Algorithm</li> </ul>"},{"location":"learn/2025/07-july/2025-07-05/","title":"July 5, 2025 - Leetcode Drill","text":""},{"location":"learn/2025/07-july/2025-07-05/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity Training</li> <li> Leetcode problem</li> </ul>"},{"location":"learn/2025/07-july/2025-07-05/#what-i-learned","title":"\ud83d\udcdd What I Learned","text":"<p>3572. Maximize Y\u2011Sum by Picking a Triplet of Distinct X\u2011Values</p>"},{"location":"learn/2025/07-july/2025-07-05/#topics-hashtable-priority-queue-array","title":"Topics: HashTable, Priority Queue, Array","text":""},{"location":"learn/2025/07-july/2025-07-05/#thinking-process","title":"\ud83d\udcdd Thinking Process:","text":"<p>The brute force solution is easy to come up with. Where we iterate through all possible combination of <code>i</code>, <code>j</code>, <code>k</code> and check if the <code>x[i]</code>, <code>x[j]</code>, <code>x[k]</code> are distinct if so then we update the maximum sum. If not we continue. This approach would take O(n<sup>3</sup>) time. This question would be very easy if all <code>x</code> values are distinct since we just need to check the top 3 largest <code>y</code> values and then take the sum and return it. So to make it distinct, we can actually iterate through all <code>x</code> values and store the highest <code>y</code> value for the corresponding <code>x</code> value. Then we iterate through all the selected <code>y</code> values and pick the top three. We can do this by pushing them onto a max heap and pick the top three elements.</p>"},{"location":"learn/2025/07-july/2025-07-05/#big-o-analysis","title":"\ud83d\udc68\u200d\ud83d\udcbb Big O analysis:","text":"<p>Assume we have n elements in the x array, and they have k distinct values. Time Complexity: O(n + klogk) Space Complexity: O(k) Reasoning: We iterate through the array x and created a hashmap with k distinct value. Then we pushed these k distinct values onto a priority queue and popped three of them.</p>"},{"location":"learn/2025/07-july/2025-07-05/#implementation-takeaways","title":"\ud83d\udeeb Implementation takeaways:","text":"<ul> <li>To easily iterate through the x and y pairs, don't iterate through the same index. Use <code>zip()</code> instead.</li> <li>Find top3 largest elements using <code>heaqp.nlargest(3, arr)</code></li> </ul> <p>1152. Analyze User Website Visit Pattern</p>"},{"location":"learn/2025/07-july/2025-07-05/#topics-array-sorting-string","title":"Topics: Array, Sorting, String","text":""},{"location":"learn/2025/07-july/2025-07-05/#thinking-process_1","title":"\ud83d\udcdd Thinking Process:","text":"<p>Given the constraints from the prompt we can see that a naive solution would be enough. What we need to do is to zip the timestamp, username and website together sort them by timestamp and group them by user. And then get the unique patterns from each user. And count the appearance for each pattern and select the one with the highest one. </p>"},{"location":"learn/2025/07-july/2025-07-05/#big-o-analysis_1","title":"\ud83d\udc68\u200d\ud83d\udcbb Big O analysis:","text":"<p>Assume user visited approximately n websites and there are k distinct usernames.</p> <ul> <li>Time Complexity: O(knlog(kn) + O(kn<sup>3</sup>))</li> <li>Space Complexity: O(kn) + O(kn<sup>3</sup>) =</li> </ul> <p>Reason: we first sort the zipped tupples and then get the combination or pattern of the websites of each user. Then we iterate through all these combinations and create a counter for each pattern. Then we pick the top one.</p>"},{"location":"learn/2025/07-july/2025-07-05/#implementation-takeaways_1","title":"\ud83d\udeeb Implementation takeaways:","text":"<ul> <li><code>combinations</code> function would preserve the original order.</li> <li>use <code>zip</code> and <code>sort</code> to sort multiple parallel arrays by only one of them.</li> <li>The <code>update</code> function </li> </ul>"},{"location":"learn/2025/07-july/2025-07-05/#a-reference-solution","title":"\ud83d\udcc3 A Reference Solution:","text":"<pre><code>    def mostVisitedPattern(self, username: List[str], timestamp: List[int], website: List[str]) -&gt; List[str]:\n\n        users = defaultdict(list)\n\n        for user, time, site in sorted(zip(username, timestamp, website), key = lambda x: (x[0],x[1])): \n            users[user].append(site)\n\n        patterns = Counter()\n\n        for user, sites in users.items():\n            patterns.update(Counter(set(combinations(sites, 3))))\n\n        return max(sorted(patterns), key=patterns.get)\n</code></pre> <p>Thanks shrey for the concise answer</p>"},{"location":"learn/2025/07-july/2025-07-05/#concepts-that-requires-further-study","title":"\ud83d\ude80 Concepts that Requires Further Study:","text":"<ul> <li>itertool library</li> </ul>"},{"location":"learn/2025/07-july/2025-07-06/","title":"July 6, 2025 - Python and RAG Study","text":""},{"location":"learn/2025/07-july/2025-07-06/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity Training</li> <li> RAG and Agents Chapter of AI Engineering</li> <li> Chapter 1 of Fluent Python</li> <li> Find interesting Github Repos</li> </ul>"},{"location":"learn/2025/07-july/2025-07-06/#what-i-learned","title":"\ud83d\udcdd What I Learned","text":""},{"location":"learn/2025/07-july/2025-07-06/#fluent-python","title":"Fluent Python","text":""},{"location":"learn/2025/07-july/2025-07-06/#python-data-model","title":"Python Data Model","text":"<p>Python data model is a system of using special methods (commonly known as magic method) to make your custom class behave like Python built in class. This is how we make custom class Pythonic. </p>"},{"location":"learn/2025/07-july/2025-07-06/#special-method","title":"Special Method","text":"<p>To enable idiomatic Python language features on our own class, special methods or magic methods are the way to go. They are usually in the format *. A good analogy of special method would be like defining api endpoint using fastapi. In fastapi, you can define some api endpoint like the following</p> <pre><code>@app.get(\"/item/{id}\")\ndef getitem(id: int, session: Session) -&gt; Item:\n    pass\n</code></pre> <p>After this, fastapi would know how to handle the http get request going to the root/item/{id}. While, we don't know what the fastapi's main event loop looks like, we know that after we define the function this would work. In terms of special method, the framework would be Python itself. After we define the dunder methods, we can enable some of the built in methods like <code>len(), in, int()</code>. Think of them as defining some API that allows the Python interpreter to know how to handle special operation with your class. Defining special method is exposing APIs to Python itself. </p>"},{"location":"learn/2025/07-july/2025-07-06/#overview-of-special-methods","title":"Overview of Special Methods","text":""},{"location":"learn/2025/07-july/2025-07-06/#collection-api","title":"Collection API","text":"<p>Here are some of the essential collection type in the language. All these classes are abstract base class which might indeed sound a bit abstract at first. </p> <p></p> <p>abc</p> <p>What are abcs essentially. You must have heard these pharses before \"a list like object, a map like object etc.\" A formal definition of \"a list like object\" is the abstract <code>Sequence</code> class in the abc module. Think of them as an interface which requires you to define certain set of methods to be qualified as certain class. For example, for some class to be considered as a <code>Sequence</code> abc, you must define <code>__getitem__</code>, and <code>__len__</code>. After you have defined these methods, the Python interpreter would implicitly consider your class as a <code>Sequence</code>. Then you can do the some of the built-in methods that takes <code>Sequence</code> as a valid input. e.g. <code>max(), sum(), sorted()</code>.</p> <p>Note</p> <p>These arrows in the diagram means the source class is a subclass of the pointed ones. So a <code>Collection</code> would be an <code>Size</code>, <code>Iterable</code>, <code>Container</code>. And <code>Sequence</code>, <code>Mapping</code> and <code>Set</code> are the most common collection types.</p>"},{"location":"learn/2025/07-july/2025-07-06/#rag-and-agent","title":"RAG and Agent","text":""},{"location":"learn/2025/07-july/2025-07-06/#why-rag-and-agent","title":"Why RAG and Agent?","text":"<p>Hallucination is one of the biggest obstacles of LLMs. It refers to the problem where LLMs would generate seemingly persuasive and yet factually inconsistent answers. The three main factors that would affect the LLM query qualities are:</p> <ul> <li>The instruction given to the model</li> <li>The context where the model is using to respond</li> <li>The model itself</li> </ul> <p>RAG and agent are attempts to improve response qualities by providing better query context. </p>"},{"location":"learn/2025/07-july/2025-07-06/#rag","title":"RAG","text":"<p>RAG is a technique that enhances a model's generation by retrieving the relavant information from external memory sources. You can think of RAG as a technique to construct context for each query instead using the same one for all the queries. </p> <p>context vs RAG</p> <p>There have been on going discussion about if RAG would still be effective after the continuous improvement on the model context length. The author of the book gives two reasons why RAG would coexist with the long context length. First of all, the context length will never be enough. Since the amount of available data would usually grow rather than decrease over time. There would always be requirements on bigger and larger context length. Secondly, a model that can process long context does not necessarily mean that it can process it well. This is one of the famous problem of LLM called \"lost in the middle\". </p>"},{"location":"learn/2025/07-july/2025-07-06/#rag-architecture","title":"RAG architecture","text":"<p>Here is an overall RAG architecture.</p> <p></p> <p>As we can see that besides from the usual user and model interaction there is are extra component retriever and external memory get involved. According to the book retriever has two main functionality indexing and querying. </p> <p>Note</p> <p>Well, after some research it seems the indexing job is usually done in the background. Especially for the initial setup phase, the vector datastore would usually takes care of that. Some framework like Haystack would wrap the encoding model and the query engine together into the retriever class. Thus you can call something like <code>retriever.index(doc)</code>. Usually, the indexing job is not handled by the retriever.</p> <p>Indexing refers to storing the documents or chunk into the vector DB and querying means embedding the user query and search the database for top k matching chunks. Sometimes the documents are just too big to be stored as one entry so the common practice is to split them into smaller chunks and then embed them and index them into the vector database.</p> <p>embedding vs indexing</p> <p>Embedding is the process of using an AI embedding models like Word2Vec, and GloVe to convert text into numerical vectors. After the conversion words with similar semantic meaning would be closer to each other in the vector space. Indexing means putting the embedded chunks into the vector database. Each of them would use different indexing techniques for faster queries.</p>"},{"location":"learn/2025/07-july/2025-07-06/#two-types-of-retrieval-algorithms","title":"Two types of retrieval algorithms","text":"<p>Term-Based Retrieval is comparatively a straightforward way to find relevant documents. It determines the relevance of a document based on the frequency a term appears in that documents. In this case, we would check for the term frequency (TF) of all terms exists in the query. This is not enough since some of the terms are more important than other terms. For example, <code>transformer architecture</code> would be way much more important than <code>for example</code> when it comes to searching for relevance. One easy and yet effective way is to find the inverse document frequency (IDF) of each term. IDF = # of all documents / # of documents that contains the term. This means the more common a term exists in all documents the less important it is. Naively, we can just find the relevance score of the document by suming all terms' TF * IDF. Iterating through all documents takes <code>O(n)</code> time. We would use inverse index instead. </p> <p>inverse index</p> <p>Inverse index maps each term to all of the documents that has it. It would be something like this: </p> <p>Embedding-Based Retrievel relies more on semantic level than lexical level. It embedds each document or chunk into numerical vector that can represent its semantic meaning. For searching, we basically embed the searching query with the same embedding model and then find its neighbors. </p> <p></p>"},{"location":"learn/2025/07-july/2025-07-06/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>\"lost in the middle\". </li> </ul>"},{"location":"learn/2025/07-july/2025-07-06/#interesting-github-repo","title":"\ud83c\udfed Interesting Github Repo:","text":"<ul> <li>Folo: An all in one media app</li> <li>MediaCrawler</li> </ul>"},{"location":"learn/2025/07-july/2025-07-07/","title":"July 7, 2025 - Setup New Anki Workflow","text":""},{"location":"learn/2025/07-july/2025-07-07/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity Training</li> <li> Setup New Learning System</li> </ul>"},{"location":"learn/2025/07-july/2025-07-07/#new-learning-system","title":"\ud83d\udcdd New Learning System:","text":"<p>Instead of the regular learning drill, I did some optimization on my learning system. I am a big fan of the famous flashcard project anki. FSRS algorithm seems to be the shortest way towards long-term memory. It has helped me get through many quizzes and exams. Despite my love for it, I found myself struggling to create a proper code block in anki flashcard -- at least, I haven't found an easy way to do it. The code is not highlighted. Tab does not work properly in anki. There is no auto indentation. Just to name a few. </p> <p>After some research online, I finally find a workaround to use Obsidian with a anki converter plugin to solve this issue. Here is its tutorial.</p> <p>Here is my end result:</p>"},{"location":"learn/2025/07-july/2025-07-07/#obsidian-view","title":"Obsidian View","text":"<p>I can create my deck through obsidian using neuracache flashcard style. And after I click the anki sync button to on the ribbon. All cards would be synced to the anki deck. </p> <p></p>"},{"location":"learn/2025/07-july/2025-07-07/#anki-view","title":"Anki View","text":"<p>With this approach, I can enjoy the code highlight of the Obsidian and easily create fully styled flashcard on Anki.</p>"},{"location":"learn/2025/07-july/2025-07-15/","title":"July 15, 2025 - Leetcode","text":""},{"location":"learn/2025/07-july/2025-07-15/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Mailmind developing</li> </ul>"},{"location":"learn/2025/07-july/2025-07-15/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-15/#where-have-i-been","title":"Where have I been:","text":"<p>It has been a week since my last log. I am still continuing my path as an AI engineerr but I find it quite hard to balance between studying through making flashcard on anki and creating content rich daily learning report. Usually, when I was making a lot of flashcards on a particular day, I was mainly learning techinical details like syntax, code patterns of certain library or language. These information does not fit well with the daily log media. It would look scattered and unorganized which I hate it. On the other hand, if I was reading a specific chapter of a book, a chapter summary would be a good log post. One of my bad habit is to have a great 'inertia' of doing things. When I was reading a book, I don't feel like developing projects that day and vice versa. I was learning quite a lot libraries these days that's why the log has been suspended for a while. I do want to get rid of this bad habit and I think writing furthers the depth of understanding. So I will try to make a daily post anyhow. (Maybe keep it minimal or just a summary)</p> <p>To verify that the tech I am studying here actually have the market. I added the following section to keep track of current market requirement of an AI engineer. </p>"},{"location":"learn/2025/07-july/2025-07-15/#industry-requirement","title":"\ud83c\udfed Industry Requirement:","text":""},{"location":"learn/2025/07-july/2025-07-15/#abbott-ai-engineerarchitect","title":"Abbott: AI Engineer/Architect","text":""},{"location":"learn/2025/07-july/2025-07-15/#job-description","title":"Job Description:","text":"<ul> <li>Integrate AI solution using LLM APIs ((OpenAI, AWS Bedrock, Anthropic, Mistral))</li> <li>Shape LLM behavior through prompt engineering, guardrail development, and fine-tuning pipelines. </li> <li>Design and build scalable APIs (e.g., FastAPI, Flask) to enable AI-driven features across products. </li> <li>Lead performance tuning and ensure system scalability across mobile and cloud platforms. </li> <li>Implement GenAI Safety Best Practices, focusing on hallucination mitigation and secure model usage. </li> <li>Testing and Debugging\u2014 proficiency in AI app testing techniques, including unit testing, integration testing, and UI testing across various mobile devices and operating system platforms.  </li> </ul>"},{"location":"learn/2025/07-july/2025-07-15/#unfamiliar-concepts","title":"Unfamiliar Concepts:","text":"<ul> <li> <p>Guardrail development: Practices that makes sure AI behaves ethically and responsibly.   </p> </li> <li> <p>AWS Bedrock: provides API for AI application development. It's Amazon's answer to OpenAI, Azure OpenAI.</p> </li> <li>Mistral AI: It is a European based AI startup founded in 2023 based in Paris. It mainly focused on developing open source LLM targeting academic, enterprise and research audience.</li> </ul>"},{"location":"learn/2025/07-july/2025-07-15/#key-takeaways","title":"Key Takeaways:","text":"<p>In general, I seem to be working on the correct direction. The tech stack of Nextjs, FastAPI, Langchain is needed on the market. Besides development and deployment, the industry also values AI testing, monitoring and guardrail practice. </p>"},{"location":"learn/2025/07-july/2025-07-15/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>design-patterns-python</li> <li>os module</li> <li>openai-python-sdk</li> <li>httpx</li> <li>openai-docs</li> <li>datetime</li> </ul>"},{"location":"learn/2025/07-july/2025-07-16/","title":"July 16, 2025 - Job requirements","text":""},{"location":"learn/2025/07-july/2025-07-16/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Mailmind developing</li> </ul>"},{"location":"learn/2025/07-july/2025-07-16/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-16/#leetcode-problem","title":"Leetcode Problem:","text":""},{"location":"learn/2025/07-july/2025-07-16/#3612-process-string-with-special-operations-i","title":"3612. Process String with Special Operations I","text":""},{"location":"learn/2025/07-july/2025-07-16/#topics-simulation-string-manipulation","title":"Topics: Simulation, String Manipulation","text":""},{"location":"learn/2025/07-july/2025-07-16/#thinking-process","title":"\ud83d\udcdd Thinking Process","text":"<p>This questions is pretty straightforward. You can just iterate through the input string and construct the return result on the go. The main problem is to optimize the string manipulation operation such as reverse, pop, append and replication.</p>"},{"location":"learn/2025/07-july/2025-07-16/#big-o-analysis","title":"\ud83d\udc68\u200d\ud83d\udcbb Big O analysis","text":"<p>Assume all operations are replication. Then there would be n operations. Then we denote the largest length of the string as 1 + 2 + 4 + ... + 2<sup>n</sup> = O(2<sup>n</sup>). For each operation. The worst case time complexity is linear time. Thus we have O(n*2<sup>n</sup>) Time Complexity: O(n * 2<sup>n</sup>) Space Complexity: O(n)</p>"},{"location":"learn/2025/07-july/2025-07-16/#implementation-takeaways","title":"\ud83d\udeeb Implementation takeaways:","text":"<p>My initial implemention used deque and implemented reverse operation as a boolean flag. Based on the flag status, we can decide which side to add, drop and replicate. However, the replication is not working as expected. Since I was using <code>deque.extendleft(reversed(deque))</code> It reports the error: <code>deque mutated during iteration</code>. The problem is that <code>reversed()</code> would generate an iterator of the list which the <code>extendleft</code> works on. During iteration the underlying deque is also mutated. In comparison <code>deque.extend(deque)</code> can work. In hindsight, this can totally be avoided since the result of <code>deque.extendleft(reversed(deque))</code> is the same as <code>deque.extend(deque)</code>. Still it is a quite interesting behavior. Also, I have learned that since slicing is optimized. It is actually quicker to just reverse the whole thing using slicing than using the boolean flag to mark the reverse state.</p>"},{"location":"learn/2025/07-july/2025-07-16/#3610-minimum-number-of-primes-to-sum-to-target","title":"3610. Minimum Number of Primes to Sum to Target","text":""},{"location":"learn/2025/07-july/2025-07-16/#topics-dp-mathsieve-of-erathosthenes","title":"Topics: DP, Math\uff0cSieve of Erathosthenes","text":""},{"location":"learn/2025/07-july/2025-07-16/#thinking-process_1","title":"\ud83d\udcdd Thinking Process","text":"<p>The prompt asks for the minimum number of primes that sums towards a target sum. Besides the primes part this is similar to a very classic dp problem: Coin change. Basically, after you have created the array of all primes that smaller the target amount. The question turned into coin change. The main idea of coin change is to think in terms of optimal solution when we are looking at a subset of coins. We create a dp array where <code>dp[i]</code> represents the optimal solution given the current set of coins. When there is a new coin added into the coins set. For <code>i</code> that is strictly smaller than the coin denomination <code>k</code>, <code>dp[0] to dp[k-1]</code> is already an optimal solution considering the new subset of coins. For <code>i</code> belongs to <code>k</code> to <code>n-1</code> we only think of whether <code>dp[i]</code> would be updated given the new coin set. i.e. if <code>dp[i - coin] + 1</code> would be smaller than the original <code>dp[i]</code>.  As suggested earlier, <code>dp[i - coin]</code> would always represent the minimum coin needed for sum <code>i - coin</code> for the new coin set. So after we incorporating all of the coin types into consideration, <code>dp[target]</code> would represent the answer.</p> <p>However, this question tested another concept: Sieve of Erathosthenes which is an algorithm that generates primes in linear time. The main idea is as following: Each composite number would be a multiple of some prime numbers that is strictly smaller than it. If we know all of the prime number that is smaller than a particular number and it is not a multiple of any of them. Then this number is definitely a prime number. Here is the key: we don't check all prime numbers that is smaller than the current candidate. Instead, we rule out the multiples of a prime number once we find one. Thus, each number would only gets checked once -- linear time complexity.</p>"},{"location":"learn/2025/07-july/2025-07-16/#big-o-analysis_1","title":"\ud83d\udc68\u200d\ud83d\udcbb Big O analysis","text":"<p>Assume the target sum is <code>n</code> and we have <code>m</code> primes.</p> <ul> <li>Time Complexity: O(n * m)</li> <li>Space Complexity: O(n)</li> </ul> <p>Since we use a dp array of size n and iterate the whole array for each prime.</p>"},{"location":"learn/2025/07-july/2025-07-16/#implementation-takeaways_1","title":"\ud83d\udeeb Implementation takeaways:","text":"<ul> <li>Understand Sieve of Erathothenes</li> <li>Know how to solve coin change problem both bottom up and top down</li> </ul>"},{"location":"learn/2025/07-july/2025-07-16/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>Kilo Code </li> </ul>"},{"location":"learn/2025/07-july/2025-07-17/","title":"July 17, 2025 - AI Engineer RAG","text":""},{"location":"learn/2025/07-july/2025-07-17/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Finished AI Engineering RAG Chapter</li> </ul>"},{"location":"learn/2025/07-july/2025-07-17/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-17/#ai-engineer-book","title":"AI Engineer Book","text":""},{"location":"learn/2025/07-july/2025-07-17/#the-comparison-between-term-based-retrieval-and-embedding-based-retrieval","title":"The comparison between term-based retrieval and embedding-based retrieval","text":""},{"location":"learn/2025/07-july/2025-07-17/#metrics-for-evaluation","title":"Metrics for evaluation","text":"<p>For retrieved documents only - Context Precision: The percentage of relevant documents among all retrieved ones - Context Recall: The percentage of retrieved documents among all relevant ones</p> <p>Document ranking</p> <p>This expects the most important documents should be retrieved first</p> <ul> <li>NDCG (normalized discounted cumulative gain)</li> <li>MAP (Mean Average Precision)</li> <li>MRR (Mean Reciprocal Rank)</li> </ul> <p>Embedding</p> <ul> <li>MTEB</li> </ul> <p>We should also evaluate the retriever with the model as a whole. The evaluation would be talked about in the future chapters</p> <p>Note</p> <p>The most time consuming part of a RAG system is actually the output generation. The embedding generation and vector search is actually minimal when compared to it.</p>"},{"location":"learn/2025/07-july/2025-07-17/#design-choice-of-retrieval-system","title":"Design choice of retrieval system","text":"<p>Mainly, we consider two parts, indexing cost and querying quality.</p> <p>To improve query quality, we use more detailed index which takes much more time and memory to create and thus getting higher query quality. One example of index of this kind is: HNSW</p> <p>To reduce index cost, we can use simpler index like LSH. This is easier to build and yet results in slower and less accurate queries.</p> <p>We also have control over the ANN algorithm used during retrieval. Check the performance for each of them here.</p>"},{"location":"learn/2025/07-july/2025-07-17/#three-aspects-of-rag-evaluation","title":"Three aspects of RAG evaluation:","text":"<ul> <li>Retrieval Quality </li> <li>Context precision, recall, query speed and accuracy, indexing efficiency</li> <li>Embedding (For embedding-based retrieval only)</li> <li>MTEB</li> <li>RAG output</li> <li>Evaluate LLM output</li> </ul>"},{"location":"learn/2025/07-july/2025-07-17/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>Shipping Fast</li> </ul>"},{"location":"learn/2025/07-july/2025-07-18/","title":"July 18, 2025 - AI Engineer Agent","text":""},{"location":"learn/2025/07-july/2025-07-18/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Started AI Engineering Agent Chapter</li> </ul>"},{"location":"learn/2025/07-july/2025-07-18/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-18/#shipping-fast","title":"Shipping Fast","text":""},{"location":"learn/2025/07-july/2025-07-18/#key-takeaways","title":"Key Takeaways:","text":"<ul> <li>Production is the only environment where your code, infrastructure, and customer come together to represent the objective reality. Only after that you can realize the problem. Shipping fast is very helpful for high performance team to iterate and learn fast.</li> <li>Ship instrumentation and first. i.e. ship the code that observes and monitors other code first before you make changes. Thus you can be aware of the specific affect your code has over the entire system.</li> <li>Be available after shipping. Get ready for on-call after shipping.</li> <li>Use feature flag to support rollout and rollback. Basically it is using a boolean or config to keep track of what features is enabled. This is actaully quite powerful since it can enable gradual rollout, targeted rollout, A/B testing and act like a kill switch, even load balancing.</li> <li>Document and share the plan and actions which means you need to create a documentation detailing how you are gonna rollout and rollback if the feature does not work out.</li> </ul>"},{"location":"learn/2025/07-july/2025-07-18/#ai-engineer-book","title":"AI Engineer Book","text":""},{"location":"learn/2025/07-july/2025-07-18/#definition-of-agent","title":"Definition of Agent","text":"<p>Agent is anything that can perceive its environment and act based upon that environment. So agent is characterized by the environment and action it can perform. </p>"},{"location":"learn/2025/07-july/2025-07-18/#relationship-between-environment-and-action","title":"Relationship between environment and action","text":"<p>The environment determines what tools a user agent can potentially use. On the other hand, the tool set that the user agent posses restricts the environment the agent can operate in.</p> <p>Why agent workflow requires more powerful models?</p> <p>Agent workflows usually involves multiple steps of reasoning and execution. The error for each step would compound causing a much higher error rate for the entire workflow. Additionally, with toolset AI would have more capabilities and thus its error would have much more impact.</p> <p>What is the difference between AI agents and agentic AI?</p> <p>The core differences between agentic AI and AI agents lies in their autonomy and decision making capabilities. AI agents usually has predefined rules and fixed routine to solve problems. It requires human input as well.</p>"},{"location":"learn/2025/07-july/2025-07-18/#ai-agent-tools","title":"AI Agent Tools","text":"<p>Knowledge Augumentation </p> <p>Examples: text, image retriever, SQL executor, web browsing</p> <p>Capability Extension</p> <p>Examples: calculator, code compiler, OCR, calendar, timezon converter, unit converter and translator.</p> <p>Sepcial extention</p> <p>If a model is text only, adding a text-to-image model as its tool can easily make it multimodal. In fact that is how ChatGPT generates pictures. It uses DALL-E internally to image generation queries.</p> <p>Write Ability</p> <p>Examples: Email APIs, database access, SQL executor, banking API. </p> <p>As AI systems become more capable, ensuring their safety becomes increasingly important. It's essential to implement guardrails to ensure that AI agents remain both powerful and responsible.</p>"},{"location":"learn/2025/07-july/2025-07-18/#planning","title":"Planning","text":"<p>The core of agent workflow is planning. Agent workflow are usually time consuming and costly. Executing the agent\u2019s plan prematurely risks wasting time and money without delivering results. Hence, there should be a dedicated planner that takes in user queries and figure out what the user want to do and generate a plan. Then either a human judge or an AI judge could evaluate the plan and decide whether or not to proceed the costly operation. Actually Copilot agent mode has exactly the same behavior. It shows all the intended changes and asks you to validate them. You also need to decouple planning from execution for the same reason. </p> <p>Intent classifier</p> <p>To better help with planning, the AI agent needs to know the intent of the user query. More often than not, there would be an independent intent classifier trained to decide what the user intent is. </p> <p>Gneral Process of Planning</p> <ol> <li>Plan generation: come up with a detailed plan where each step is manageable.</li> <li>Reflection and error correction: evalute the generated plan and fix the error if exists</li> <li>Exeuction: take the action outlined in the plan.</li> <li>Reflection and error correction: this time we reflection on the result of the executed plan and decide if the execution has reached our goal. If not we need to correct the error and generate a new plan.</li> </ol> <p>Arguments about whether FM know planning at all</p> <p>There has been ongoing debate over whether autoregressive models are truly capable of planning. Critics argue that planning fundamentally involves searching for an optimal path to a goal by evaluating and comparing different options\u2014often requiring backtracking. Backtracking allows the system to recognize when the current path is suboptimal and to revise earlier decisions accordingly. In contrast, an autoregressive model generates output by predicting the next token based solely on previous ones, without the ability to revise earlier choices, which seems incompatible with true planning.</p> <p>In response, some counterarguments suggest that autoregressive models can exhibit a form of planning by regenerating the entire sequence with different decisions if the initial plan is deemed ineffective. While this does not involve traditional backtracking, it can still lead to improved outcomes through iterative refinement.</p>"},{"location":"learn/2025/07-july/2025-07-18/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>Solo developer workflow</li> </ul> <p>Some useful tips: </p> <ul> <li>design the UI first (use AI to speed up prototyping). For UI ideas look for competitors. </li> <li>Build application that is market proven. (Already exists in the market). </li> <li>Summarize competitors' pros and cons and create your own style based on your understanding. </li> <li>Use MCP server to expand AI capability.</li> <li>Prompt engineering: \"Do not make any changes until you have 95% confidence that you know what to build ask me follow up questions until you have that confidence. Add this as cursor rule.</li> <li> <p>Be careful about creating new chat since you might loose context.</p> </li> <li> <p>Kiro:  Amazon's new AI IDE focuses on specification-driven development. It addresses a key pain point: when using prompts, it's difficult to understand the assumptions the AI makes during system design, making it hard to interpret or document the output. Kiro solves this by using specifications. Instead of relying on prompts, it guides the AI with clear, structured requirements to ensure accurate implementation. After each prompt, Kiro would first generate a spec in the <code>.kiro</code> folder. It would contain system design and a detailed round map of implementation. You can validate and modify it before Kiro actually modify the code. It also features hook, which is a event driven background agent. For example, you can create a hook so that when a new component is created it would generate a test file for that component. And whenever a file gets deleted, you can also create a hook to let kiro run existing tests to see if things broke.</p> </li> <li> <p>Ken Wilder. Study his AQAL model sometime in the future.</p> </li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/","title":"July 19, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-19/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Finish AI Engineering Agent Chapter</li> <li> Leetcode</li> <li> Developing pdf-chat</li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-19/#leetcode-problem","title":"Leetcode problem","text":"<p>22. Generate Parentheses</p>"},{"location":"learn/2025/07-july/2025-07-19/#thinking-process","title":"Thinking Process","text":"<p>I saw the limitation of the string length is at most 8 and I jumped right to the brutal force. The main idea is to iterate through all possible string with length <code>2n</code> and check if it is well-formed. To generate, a combination of parenthesis string with length <code>2n</code>. I iterate through number 2^(2n - 1)^ to 2<sup>(2n)</sup> - 1 and mapped the binary bits 1 to '(', 0 to ')'. Or we can use BFS to generate all possible combination. Then we just check if the string is a valid one and append it to the output array. In hindsight, using backtrack can prune the search tree easily. Instead of using a BFS, we can do a DFS and check if the current string is a valid one by checking if its left parenthesis count is smaller or equal to <code>n</code> and if the left parenthesis is not less than the right parenthesis count. </p>"},{"location":"learn/2025/07-july/2025-07-19/#useful-code-snippet","title":"Useful code snippet","text":"<p>Check if a string is well-formed parentheses. <pre><code>def check_parentheses(s: str) -&gt; bool:\n    left_cnt = 1\n    for c in s:\n        if c == \"(\":\n            left_cnt += 1\n        else:\n            left_cnt -= 1\n        if left_cnt &lt; 0:\n            return False\n    return True\n</code></pre></p> <p>Generate all possible combiniation with bfs <pre><code>from collections import deque\n\n\ndef generate_all_comb(n: int) -&gt; list[str]:\n    queue = deque([\"\"])\n    ans = []\n    while queue:\n        cur_str = queue.popleft()\n        if len(cur_str) == n:\n            ans.append(cur_str)\n            continue\n        queue.append(cur_str + \"(\")\n        queue.append(cur_str + \")\")\n    return ans\n</code></pre></p> <p>Check if a number <code>n</code>'s <code>x</code>th bit is 1 'n &gt;&gt; (x - 1) &amp; 1 == 1'</p>"},{"location":"learn/2025/07-july/2025-07-19/#complexity-analysis","title":"Complexity Analysis","text":"<p>Denote the length of the string as <code>n</code></p> <p>For brutal force: Time Complexity: O(2<sup>2n</sup>*n)</p> <p>Space Complexity: O(2<sup>2n</sup>*n)</p> <p>Reasoning: We iterate through all of the combination of parenthesis string with length <code>2n</code> that would be 2<sup>2n</sup> and for each string we need O(n) time to check if it is valid thus. O(2<sup>2n</sup>*n) for time. For the space, the queue would be largest when it reaches the bottom. It would have 2<sup>2n</sup> string in it each with length 2n. Thus O(2<sup>2n</sup>n) for space complexity.</p>"},{"location":"learn/2025/07-july/2025-07-19/#key-takeaway","title":"Key Takeaway","text":"<ul> <li>How to check valid parenthesis string</li> <li>How to all possible combination through BFS and DFS</li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/#ai-engineer-book","title":"AI Engineer Book","text":"<p>Last time we studied the planning part of the AI agent workflow. So far the system has three components or agents: one to generate plan, one to evaluate plan and another to execute plan.</p>"},{"location":"learn/2025/07-july/2025-07-19/#how-to-generate-a-plan","title":"How to generate a plan","text":"<p>First, we need to know that the easiest way to turn a FM into a planner is through prompt engineering. Here is a sample prompt that does that trick:</p> <p><pre><code>SYSTEM PROMPT\nPropose a plan to solve the task. You have access to 5 actions: \nget_today_date()\nfetch_top_products(start_date, end_date, num_products)\nfetch_product_info(product_name)\ngenerate_query(task_history, tool_output)\ngenerate_response(query)\n\nThe plan must be a sequence of valid actions.\n\nExamples \nTask: \"Tell me about Fruity Fedora\" \nPlan: [fetch_product_info, generate_query, generate_response]\n\nTask: \"What was the best selling product last week?\" \nPlan: [fetch_top_products, generate_query, generate_response]\n</code></pre> Notice, we are ignoring the parameters for the function in the plan set. The exact parameters are hard to predict in advance since they are often extracted from the tool output. Now with this prompt, the model would know the tool set it got and the plan format.</p> <p>Hallucination is always the biggest enemy of AI application. Here hallucination might happen when the user prompt or task is not clear enough. For example, we might as about \"What is the average price of the top products?\". The prompt here does not mention any of the parameter required for the function <code>fetch_top_products</code>. Do we want all of the top products or just 5? Do we want the top products within this week or month? Since we did not provide these information, the model is going to predict it by itself. And that's where hallucination comes.</p> <p>Some techniques to solve this involves:</p> <ul> <li> <p>Make it easier for AI to use the tools</p> <ul> <li>Write a better system prompt with more examples. </li> <li>Give better descriptions of the tools and their parameters so that the model understands them better.</li> <li>Rewrite the functions themselves to make them simpler, such as refactoring a complex function into two simpler functions.</li> </ul> </li> <li> <p>Improve the model</p> <ul> <li>Use a stronger model. In general, stronger models are better at planning. </li> <li>Finetune a model for plan generation.</li> </ul> </li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/#function-calling","title":"Function calling","text":"<p>Tools are often created as a function for models to call. Thus using tools is called \"function calling\". </p> <p>Create function calling involves two steps in general:</p> <ul> <li>Create a tool inventory (i.e. define a list of function with its function name, parameters and documentation)</li> <li>Specify what tools the agent can use (This sounds a bit weird, since we can give the model access to all tools available and let itself decide. However, letting people to choose the tools set would help alliveate the hallucination problem)</li> </ul> <p>An example of function calling: </p> <p>Debug function calling</p> <p>When working with agent always tell the system to output the arguments they use</p>"},{"location":"learn/2025/07-july/2025-07-19/#plan-granularity","title":"Plan granularity","text":"<p>When creating a plan there is a trade-off between plan granularity and execution. More detailed or granular a plan is, the easier to execution. However, a granular plan is both hard to generate and to maintain. In the previous prompt example, the model would generate a plan in the format of a list of functions which is very detailed. The trade-off is if the function name or parameter changes, you have to update the examples and prompt as well. Also, your planner won't work on different APIs. Even worse, if you fine-tuned your model for these tools, you need to fine-tune it again for new functions. In general, it would be better to tell the models to generate plans in natural languages. Thus, it is not too hard to execute but also easier to maintain.</p>"},{"location":"learn/2025/07-july/2025-07-19/#complex-plan","title":"Complex Plan","text":"<p>Complex plan refers to plans that is not sequential. You can add control flow, loop into it and even add parallel workflow. The order of which actions is executed is called control flow. </p> <p>Four types of control flow: Sequential, If statement, Parallel, For Loop</p> <p>Let's clarify on the Parallel control flow a bit. One example could be loop for the price of top 100 best selling product. The workflow should be: first find all top 100 best selling product. Then retrieve the price of each of them. Parallel control workflow can optimize the process of getting the price of each product by creating subtasks or subagent to retrieve the price concurrently. Usually, you can apply this pattern when you need to first fetch a bunch of result and perform some action on each of them. I think the deep research feature of these popular models uses parallel control flow. Models would create subagent to fetch different websites simulaneously to redunce latency.</p>"},{"location":"learn/2025/07-july/2025-07-19/#reflection-and-error-correction","title":"Reflection and error correction:","text":"<p>When to reflect?</p> <ul> <li>Validate is a query is feasible</li> <li>Check if the generated plan is good enough</li> <li>Evaluate each step during the workflow execution to make sure it is on the right track</li> <li>Determine if the final output satisfied the user's need</li> </ul> <p>To achieve reflection, we can add an extra model as a scorer or use a self-reflecting prompt.</p> <p>One of the famous example is ReAct framework. The framework tells the model to interleave reasoning and action where reasoning is essentially planning and reflection. The agents is told to reason first, then take action and finally analyze its observation. While the initial ReAct framework is just a prompt engineering that allows a single model to think and act in multiple steps. In our multi-agent system, each task could be delegated to each agent.</p> <p>Pros and Cons of reflection</p> <p>Reflection is comparatively easy to implement and have surprisingly good performance improvement. Nevertheless, it would introduce more latency and costs. Since thoughts, observation and actions are expensive to generate. To make sure the model would generate in the format of thought, action and reflection. Model creators usually need to use a plenty of examples in the prompt. This would also reduces the context space available for other information.</p>"},{"location":"learn/2025/07-july/2025-07-19/#tool-selection","title":"Tool selection","text":"<p>To select the best tool set, it is mostly trials and errors. Here are some apporaches that might help you find best tool set</p> <ul> <li>Ablation test</li> <li>Compare model performance with different tool set</li> <li>Look for tools that agent frequently make mistake on</li> <li>Plot the distirbution of tool calls to see what tools are most used and what are least used</li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/#agent-failure-modes-and-evaluation","title":"Agent Failure Modes and Evaluation","text":"<p>Planning Failure</p> <p>Planning failure is often associated with tool use common errors are:</p> <ul> <li>Invalid tool</li> <li>Valid tool, invalid parameters</li> <li>Valid tool, incorrect parameter values</li> </ul> <p>Another type is goal failure: the agent fails to achieve the goal. The plan might fail to solve a task or fail to solve under the contraint given in the prompt.</p> <p>A special type of plan failure is caused by error in the reflection. Remember how we need to use reflect to check if a plan is valid or not. If the reflection itself is problematic, the agent migth be convinced that it has accomplished a goal while it hasn't. </p> <p>To properly evaluate what is causing the plan failure we can use the following metric: </p> <p>Create a planning dataset where each example is a tuple (task, tool inventory). For each task, use the agent to generate a K number of plans. Compute the following metrics:</p> <ol> <li>Out of all generated plans, how many are valid?</li> <li>For a given task, how many plans does the agent have to generate, on average, to get a valid plan?</li> <li>Out of all tool calls, how many are valid?</li> <li>How often are invalid tools called?</li> <li>How often are valid tools called with invalid parameters?</li> <li>How often are valid tools called with incorrect parameter values?</li> </ol> <p>Analyze the pattern and hypothesize what is causing the problem.</p>"},{"location":"learn/2025/07-july/2025-07-19/#tool-failure","title":"Tool Failure","text":"<p>Tool failures only happen when a correct tool is choosen by the plan and yet the output is wrong. From here we can easily see that there are two places things could go wrong. One is the tool itself, the other is transaltor that translate the high level plan into executable commands. </p> <p>To debug, we need to test each tool independently and make sure the model have resource or credential to use that tool. For translator we might mimic how we evaluate the planner. We can test the translator with a list high level plans and check its output. </p>"},{"location":"learn/2025/07-july/2025-07-19/#efficiency","title":"Efficiency","text":"<p>To check efficiency, we need to know the following:</p> <ul> <li>How many steps does the agent need, on average, to complete a task? </li> <li>How much does the agent cost, on average, to complete a task?</li> <li>How long does each action typically take? Are there any actions that are especially time-consuming or expensive?</li> </ul>"},{"location":"learn/2025/07-july/2025-07-19/#memory","title":"Memory","text":"<p>Memory is essential for model to retain and utilize informaion and is more useful for knowledge-rich application like RAG and agents. </p> <p>There are three types of memory mechanism:</p> <ul> <li>Internal knowledge (the knowledge captured by model parameters)</li> <li>Short-term memory (Model context window)</li> <li>Long-term memory (extra datasource model have access to, RAG system basically)</li> </ul> <p>Some pros for adding external memory system to AI models:</p> <ul> <li>Prevent system context overflow</li> <li>Smooth UX between session</li> <li>Ensure answer consistency (It could remember the answer from last time)</li> <li>Maintain data structural integrity (text data is unstructured, if we use information from a external table things would be different)</li> </ul> <p>Memory system consists of two functionality: </p> <ul> <li>Memory management (decides on which memory should go into long-term and which should be in short-term)</li> <li>Memory retrieval (fetch info from long-term memory)</li> </ul> <p>Memory retrieval is similar to RAG system so we only illustrate how memory management work. Essentially, memory management only have two operations: add and delete. Knowing which memory to add and which memory to delete is the key. </p> <p>Context Window</p> <p>For each query, if the responding to the query requires external knowledge, the model would reserve some part of the context windowf for retrieved external knowledge. Knowing that context window is shared by both short-term and long-term memory helps you form an accurate mental model of it. Also, when the short-term memory overflows due to limited context window. It is a common practice to temporarily store them in the long-term memory.</p> <p>An easiest way of managing memory is FIFO order. We delete the outdated memory for fresh ones. This is based on the assumption that early messages are less relevant to the current conversation which is not necessarily correct and in some cases severely wrong. More sophisticated way is to compress the memory or removing redundancy. A good approach is to make a summerization over the information. Look for Memory Management through Summarization for more detail. There is also a reflection approach. </p>"},{"location":"learn/2025/07-july/2025-07-19/#mcp","title":"MCP","text":"<p>MCP is an open source protocol that standardized how model can connect to external resources which includes but not limits to database, code repo and APIs. To be more specific, it defined a structured interface for: </p> <ul> <li>Tool calling</li> <li>Schema based input and output</li> <li>Authentication and access control</li> <li>Context-aware invoation</li> </ul> <p>The main architecture:</p> <p></p> <p>You can see that MCP has two main parts: **MCP server and MCP clients\"</p> <p>MCP Server can be considered as a smart adapter for a tool or an app. It is managed by whoever is providing the tools and application. For example, github can create MCP server that allows AI to create a PR and resolve and issue. youtube can create an MCP server that allows AI to subscribe to channels and download videos. </p> <p>MCP server components and its purpose:</p> Component Purpose Tool schemas Define tool name, description, input/output schema (often in OpenAPI/JSON Schema) Invocation endpoint A standard HTTP endpoint that executes the tool with provided arguments Manifest/registry A file or endpoint that lists all available tools and their specs Authentication Handles access control for who can call the tool (e.g. API keys, OAuth) Metadata Describes version, owner, categories, etc. <p>MCP client is simply a software component or a layer that connects to MCP server reads the MCP tool list, registers them to the AI and handles tool calling when needed.</p> <p>For a more detailed version:</p> Step Function 1. Discovery Fetches the list of tools from a remote MCP server (e.g. via a manifest endpoint or OpenAPI spec) 2. Schema Parsing Parses each tool\u2019s input/output schema and description (JSON Schema or OpenAPI) 3. Expose to Model Formats the tools into a format the model understands (e.g., OpenAI function spec, JSON tool-call input) 4. User Query Handling Sends the user query and available tool specs to the LLM 5. Tool Selection &amp; Arguments Lets the LLM choose a tool and generate arguments 6. Invoke Tool Makes an actual HTTP call to the MCP server\u2019s execution endpoint 7. Return Result Sends the tool\u2019s response back to the model as an observation for further reasoning or final answer <p>You can find MCP client in the AI application like Microsoft Copilot, Cursor etc.</p>"},{"location":"learn/2025/07-july/2025-07-19/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>ReAct</li> <li>Paper with Code</li> <li>Chain-of-thought</li> <li>Gorilla</li> <li>AutoGPT</li> <li>Composio</li> <li>Berkely Function Calling Leaderboad</li> <li>AgentOps evaluation harness</li> <li>TravelPlanner benchmark</li> <li>Memory Management through Summarization</li> <li>Memory Management through Reflection</li> </ul>"},{"location":"learn/2025/07-july/2025-07-20/","title":"July 20, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-20/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing pdf-chat</li> </ul>"},{"location":"learn/2025/07-july/2025-07-20/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-20/#leetcode-problem","title":"Leetcode problem","text":"<p>Solved again using backtracking and divide and conquer</p> <p>22. Generate Parentheses</p>"},{"location":"learn/2025/07-july/2025-07-20/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>How to define a valid parenthesis string recursively: \"(\" + left_valid + \")\" + right_valid</li> </ul> <p>875. Koko Eating Bananas</p>"},{"location":"learn/2025/07-july/2025-07-20/#thinking-process","title":"Thinking Process","text":"<p>From the constraint we can see that <code>h</code> is at least equal to the length of <code>piles</code>. Thus, it is always possible for koko to finish in time. Ask for clarification if this is not included in the prompt. We can also infer that the largest minimal speed possible is the <code>max(piles)</code>. A native solution would be iterate through <code>1</code> to <code>max(piles)</code> and see which one satisfies the requirement. For better runtime, we can do a binary search that set the initial speed at <code>(1 + max(piles)) // 2</code>. In <code>log(n)</code> time we can find the minimum viable speed.</p>"},{"location":"learn/2025/07-july/2025-07-20/#complexity-analysis","title":"Complexity Analysis","text":"<p>Denote the maximum of the piles as <code>m</code> and the length of piles as <code>n</code>.</p> <p>Time Complexity: O(nlog(m)) Space Complexity: O(1)</p>"},{"location":"learn/2025/07-july/2025-07-21/","title":"July 21, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-21/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> </ul>"},{"location":"learn/2025/07-july/2025-07-21/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-21/#leetcode-problem","title":"Leetcode problem","text":"<p>79. Word Search</p>"},{"location":"learn/2025/07-july/2025-07-21/#thinking-process","title":"Thinking Process","text":"<p>Another backtrack problem. Just iterate through all cells and perform a dfs search on each of them. If there is a match we return <code>True</code>. If none, we return <code>False</code>.</p>"},{"location":"learn/2025/07-july/2025-07-21/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Notice sometimes you can reuse the input 2D matrix as a visited matrix. Here you can mark any cell that you have visited as <code>#</code>. </li> <li>Remember to restore the visited marker when you backtrack.</li> </ul>"},{"location":"learn/2025/07-july/2025-07-21/#complexity-analysis","title":"Complexity Analysis","text":"<p>Denote the number of cells as <code>n</code> and the length of word as <code>L</code>.</p> <p>Time Complexity: O(n*3<sup>LL</sup>) Space Complexity: O(L)</p> <p>We iterate through all cells to perform a DFS. The DFS has at most 3<sup>L</sup> calculation, since we have three direction to go at each level and there is L level at most. For the space, we only uses the recursion stack which has the same depth as the word.</p>"},{"location":"learn/2025/07-july/2025-07-21/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":""},{"location":"learn/2025/07-july/2025-07-22/","title":"July 22, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-22/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> </ul>"},{"location":"learn/2025/07-july/2025-07-22/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-22/#leetcode-problem","title":"Leetcode problem","text":"<p>1249. Minimum Remove to Make Valid Parentheses</p>"},{"location":"learn/2025/07-july/2025-07-22/#thinking-process","title":"Thinking Process","text":"<p>First, we need to think of how a string is considered valid. If you have solved valid parenthesis problem before. You would remember we use a left counter to keep track of how many left parenthese there are in the current string prefix. If any string prefix has the invariant where left count is always greater or equal to zero, then we are half way to a valid string. We only need to check if the left coutner is zero when we reach the end of the string. In other word, to make a string valid, we need to get rid of any \")\" that makes the left counter smaller than zero. This process would get rid of any \")\" that is extra. What about the \"(\"? We can do another pass in the reverse direction and keep a right counter for the \")\" paranthesis. After two passes we can finally get a valid string with minimal removal. </p>"},{"location":"learn/2025/07-july/2025-07-22/#implementation","title":"Implementation","text":"<pre><code>class Solution:\n    def minRemoveToMakeValid(self, s: str) -&gt; str:\n        ret = list(s)\n        left_cnt = 0\n        for idx, c in enumerate(s):\n            if c == \"(\":\n                left_cnt += 1\n            elif c == \")\":\n                if left_cnt == 0:\n                    ret[idx] = \"\"\n                    continue\n                left_cnt -= 1\n        right_cnt = 0\n        for idx in range(len(ret) - 1, -1 ,-1):\n            c = ret[idx]\n            if c == \")\":\n                right_cnt += 1\n            elif c == \"(\":\n                if right_cnt == 0:\n                    ret[idx] = \"\"\n                    continue\n                right_cnt -= 1\n        return \"\".join(ret)\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-22/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>how to check if a string is valid</li> <li>use inplace modification for O(1) space</li> <li>understand what does the left and right counter really represent</li> </ul>"},{"location":"learn/2025/07-july/2025-07-22/#complexity-analysis","title":"Complexity Analysis","text":"<p>Denote the length of the string as <code>n</code></p> <p>Time Complexity: O(n) Space Complexity: O(1)</p> <p>314. Binary Tree Vertical Order Traversal</p>"},{"location":"learn/2025/07-july/2025-07-22/#thinking-process_1","title":"Thinking Process","text":"<p>My intuition is to create a perform a BFS on all the node and associate each node to a column list. There should be a container wrapper on this column list object where it knows what is its previous column list at and the next column list at. Since we need to further travese the <code>node.left</code> and <code>node.right</code>. Naturally, I think of a doubly linked list. By putting a list as a value in a doubly linked node, it can easily traverse forward and backward. One last problem is that how to keep track of the head of the doubly linked list. The candidate of the head of the linked list is always created when certain link node does not have a prev node and yet we need to traverse left. Whenever that happens we mark the node as the head. Here is my initial implementation.</p>"},{"location":"learn/2025/07-july/2025-07-22/#implementation_1","title":"Implementation","text":"<pre><code>from collections import deque\n\n\nclass LinkNode:\n    def __init__(self, lst=None, prev=None, next=None):\n        self.lst = lst if lst else []\n        self.prev = prev\n        self.next = next\n\nclass Solution:\n    def verticalOrder(self, root: Optional[TreeNode]) -&gt; List[List[int]]:\n        if not root:\n            return []\n        dummy_head = LinkNode()\n        cur_node = LinkNode()\n        dummy_head.next = cur_node\n        queue = deque([(root, cur_node)])\n        while queue:\n            node, link_node = queue.popleft()\n            link_node.lst.append(node.val)\n            if node.left:\n                if not link_node.prev:\n                    link_node.prev = LinkNode(next=link_node)\n                    dummy_head.next = link_node.prev\n                queue.append((node.left, link_node.prev))\n            if node.right:\n                if not link_node.next:\n                    link_node.next = LinkNode(prev=link_node)\n                queue.append((node.right, link_node.next))\n        ret = []\n        cur = dummy_head.next\n        while cur:\n            ret.append(cur.lst)\n            cur = cur.next\n        return ret\n</code></pre> <p>In hindsight, we can also use a <code>defaultdict(list)</code> to achieve the same effect. We can simply let the root node's column list to be list 0 and decrease the index when we traverse left and increase when traversing right. Also, keep track of the smallest index, since we need it when returning.</p>"},{"location":"learn/2025/07-july/2025-07-22/#complexity-analysis_1","title":"Complexity Analysis","text":"<p>Denote the number of all nodes as <code>n</code></p> <p>Time Complexity: O(n) Space Complexity: O(n)</p> <p>Since we traverse through all node once using BFS and create a replication of all node values in the column lists.</p>"},{"location":"learn/2025/07-july/2025-07-22/#lanchain","title":"Lanchain","text":"<p>What are langchain's main functionality:</p> <ul> <li>API encapsulation</li> <li>Structuring LLM output</li> <li>Tool incorporation</li> <li>Memory Management</li> <li>RAG, Agent Development</li> <li>Server and API release</li> </ul>"},{"location":"learn/2025/07-july/2025-07-22/#chain","title":"Chain","text":"<p>Definition</p> <p>A chain is an abstraction that defines a sequence of steps to process input and generate output using one or more language models</p> <p>Basic Example <pre><code>from langchain_core.output_parsers import StrOutputParser\nfrom langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\nbasic_qa_chain = model | StrOutputParser()\n\nquestion = \"introduce yourself\"\nresult = basic_qa_chain.invoke(question)\n</code></pre></p> <p>Use the <code>|</code> pipeline symbol for chaining steps</p> <p>The classic one would also involve predefined prompt</p> <pre><code>from langchain_core.output_parsers import StrOutputParser\nfrom langchain.chat_models import init_chat_model\nfrom langchain.prompts import ChatPromptTemplate\n\nmodel = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\nprompt_template = ChatPromptTemplate([\n    (\"system\", \"you are a helpful asssitant, please answer based on users' questions),\n    (\"user\", \"This is user's question: {topic}, please answer with 'yes' or 'no')\n])\nbool_qa_chain =  prompt_template | model | StrOutputParser()\n\nquestion = \"introduce yourself\"\nresult = bool_qa_chain.invoke(question)\n</code></pre> <p>There are quite a lot of different output parser available for langchain. However, as more and more models supports function calling these parsing steps are done automatically by the function calling tools. </p>"},{"location":"learn/2025/07-july/2025-07-22/#prompt-templates","title":"Prompt Templates","text":"<p>Prompt templates takes in a dictionary where each key represents a variable in the prompt template to fill in and then returns a <code>PromptValue</code> object which is passed down into the LLM or ChatModel downstream. <code>PromptValue</code> can be useful to switch between <code>Message</code> type and the <code>str</code>.</p> <p>To create a <code>PromptTemplate</code></p>"},{"location":"learn/2025/07-july/2025-07-22/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study:","text":"<ul> <li>Google ADK</li> <li>Deep Flow</li> <li>lmdeploy</li> <li></li> </ul>"},{"location":"learn/2025/07-july/2025-07-23/","title":"July 23, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-23/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-23/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-23/#leetcode-problem","title":"Leetcode Problem","text":"<p>339. Nested List Weight Sum</p>"},{"location":"learn/2025/07-july/2025-07-23/#thinking-process","title":"Thinking Process","text":"<p>Well the question is a bit straightforward, we need to flatten the nested integer lists and add the product of integer value and its depth to a total. Intuitively, we just need to perform a BFS on the nested integers. If the object is a list, we expand it and append it to the queue along with its depth. If it is an integer, we multiply it along with its depth and add to the total sum.</p>"},{"location":"learn/2025/07-july/2025-07-23/#implementation","title":"Implementation","text":"<pre><code>from collections import deque\n\nclass Solution:\n    def depthSum(self, nestedList: List[NestedInteger]) -&gt; int:\n        queue = deque([])\n        for ni in nestedList:\n            queue.append((ni, 1))\n        total = 0\n        while queue:\n            ni, depth = queue.popleft()\n            if ni.isInteger():\n                total += ni.getInteger() * depth\n            else:\n                for new_ni in ni.getList():\n                    queue.append((new_ni, depth + 1))\n        return total\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-23/#complexity-analysis","title":"Complexity Analysis","text":"<p>Let all there be <code>n</code> integers. Time Complexity: O(n) Space Complexity: O(n)</p>"},{"location":"learn/2025/07-july/2025-07-23/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Redo it using DFS</li> </ul>"},{"location":"learn/2025/07-july/2025-07-23/#ai-engineering","title":"AI Engineering","text":""},{"location":"learn/2025/07-july/2025-07-23/#ambient-agent","title":"Ambient Agent","text":"<p>Definition: Ambient agents are agents that listens to an event stream and act on it accrodingly, potentially acting on multiple tasks.</p> <p>There are two attributes that are important:</p> <ul> <li>Triggered by event (not necessarily human interaction)</li> <li>Parallelism (being able to run multiple tasks simultaneously)</li> </ul>"},{"location":"learn/2025/07-july/2025-07-23/#human-in-the-loop","title":"Human-in-the-loop","text":"<p>Definition: Huamn-in-the-loop pattern refers to AI system where human interaction is involved to enhance the system accuracy. </p> <p>Here for ambient agent the most popular human-in-the-loop pattern contains three parts: </p> <ul> <li>Notify</li> <li>Question</li> <li>Review</li> </ul> <p>The notify step tells the user what background event is happening which is usually worth attention. No steps are taken by AI at this phase. One good example would be AI trading assistant tells you that there is a new stock that is trending, but it won't buy on your behalf.</p> <p>The question phase asks user question only when it is unclear to AI how to proceed to solve the event. It might be that AI does not have the tooling, lacks relevant information, or does not have the credential to perform the action. A good example would be that you received an invitation email to a hackathon and the AI agent is trying to reply on behalf of you but it does not know if you want to attend this event or not.</p> <p>The last step is the review. Usually we don't want AI to take actions that seems \"dangerous\" like replying to an email, buying, selling stocks. Yet still, we want them to do it for us only after we have reviewed their work just like how a real assistant work. </p> <p>Advantages of HITL:</p> <ul> <li>The most natural way how human interact with the AI assistant, like a real assistant</li> <li>Lower the stake of AI agents</li> <li>Continuous learning and training, personalization with long-term memory</li> </ul>"},{"location":"learn/2025/07-july/2025-07-23/#prompt-template","title":"Prompt Template","text":""},{"location":"learn/2025/07-july/2025-07-23/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>ambient agents</li> <li>langchain-email-assistant</li> </ul> <ul> <li>Devin</li> </ul>"},{"location":"learn/2025/07-july/2025-07-24/","title":"July 24, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-24/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-24/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-24/#leetcode-problem","title":"Leetcode Problem","text":"<p>61. Rotate List</p>"},{"location":"learn/2025/07-july/2025-07-24/#thinking-process","title":"Thinking Process","text":"<p>The key to the problem is to figure out what does rotate <code>k</code> steps mean. Take an example, given a linked list with length of <code>n</code> rotate <code>k</code> would mean making the <code>n - (k % n)</code>th (0 based index) node as the new head and <code>n - (k % n) - 1</code>th node the new tail. Now the question can be broken into two parts. First closed the linked list and get the <code>n</code>. Then move to the <code>n - (k % n) - 1</code>th node, break the link and return the new head.</p>"},{"location":"learn/2025/07-july/2025-07-24/#complexity-analysis","title":"Complexity Analysis","text":"<p>Time Complexity: O(n) Space Complexity: O(1)</p>"},{"location":"learn/2025/07-july/2025-07-24/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>question like this is all about translation. Rotate <code>k</code> -&gt; find <code>n - (k % n) - 1</code>th node</li> </ul> <p>3623. Count Number of Trapezoids I</p>"},{"location":"learn/2025/07-july/2025-07-24/#thinking-process_1","title":"Thinking Process","text":"<p>First we need to think of how we can get a unique horizontal trapezoid. There are many points on the plane some of them fall on the same horizontal line. We need to pick two different horizontal lines and pick two different points on that line. Four points picked like this would form a unique horizontal trapezoid. Thus the information that is useful to us are the following: the number of points on each horizontal line, the number of combination of picking two different points from each horizontal line, and finally the combination of four points from two different lines. We are given that there are a points lying on the line y = 1, which form x unique pairs. Similarly, there are b points on the line y = 3 forming y pairs, and c points on y = 5 forming z pairs. Then the total number of combination would be xy + xz + yz. If we calculate this using nested for loops, in the worst case, we need O(n<sup>2</sup>) time. To avoid this we can calculate the (x + y + z)<sup>2</sup> and subtract it with x<sup>2</sup> + y<sup>2</sup> + z<sup>2</sup> then divide it by 2. This would only cost liner time in worst case.</p>"},{"location":"learn/2025/07-july/2025-07-24/#complexity-analysis_1","title":"Complexity Analysis","text":"<p>Time Complexity: O(n) Space Complexity: O(n)</p>"},{"location":"learn/2025/07-july/2025-07-24/#langchain","title":"Langchain","text":""},{"location":"learn/2025/07-july/2025-07-24/#runnable","title":"Runnable","text":"<p>Runnable interface is one of the most important interface in langchain since defines the foundation of how to work with LangChain components. Language model, output parser, retriever and compiled LangGraph all implements this interface. </p> <p>A Runnable component can be </p> <ul> <li>Invoked: A single input is transformed into an output.</li> <li>Batched: Multiple inputs are efficiently transformed into outputs.</li> <li>Streamed: Outputs are streamed as they are produced.</li> <li>Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.</li> <li>Composed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.</li> </ul> <p><code>`` note \"batch mode\"     The batch mode of Runnable is achieved through</code>threading` module thus it is not actually concurrent due to the GIL. However, this is still useful since the calls to LLM are I/O bound, and the thread would be put on hold if it hits an I/O operation.</p>"},{"location":"learn/2025/07-july/2025-07-24/#lcel","title":"LCEL","text":"<p>To understand runnable we need to first understand how langchain expression language work. It is a new language designed to better manage Runnables. It takes an declarative approach to building new Runnable from existing ones.</p> <p>Here is a cheatsheet. From it you can tell what are some common interaction with runnables using LCEL.</p> <p>When to use LCEL? Here is the guideline:</p> <ul> <li>If you are making a single LLM call, you don't need LCEL; instead call the underlying chat model directly.</li> <li>If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, - if you're taking advantage of the LCEL benefits.</li> <li>If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.</li> </ul> <p>Composition Syntax:</p> <p>The two primitive of LCEL is <code>RunnableSequence</code> and <code>RunnableParallel</code>. From the name you can tell that one specifies the runnable to run sequentially and the other in parallel.</p> <p>You can create them using class constructor or use their shorthand.</p> <p><code>chain = runnable1 | runnable2</code> is equivalent to <code>chain = RunnableSequence([runnable1, runnable2])</code></p> <p>Inside an LCEL expression, a dictionary is automatically converted to a <code>RunnableParallel</code>.</p> <pre><code>mapping = {\n    \"key1\": runnable1,\n    \"key2\": runnable2,\n}\n\nchain = mapping | runnable3\n</code></pre> <p>The above is converted to </p> <p><code>chain = RunnableSequence([RunnableParallel(mapping), runnable3])</code></p> <p>Inside an LCEL expression, a function is automatically converted to a <code>RunnableLambda</code>.</p> <pre><code>def some_func(x):\n    return x\n\nchain = some_func | runnable1\n</code></pre> <p>The above is converted to</p> <p><code>chain = RunnableSequence([RunnableLambda(some_func), runnable1])</code></p>"},{"location":"learn/2025/07-july/2025-07-24/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>GIL</li> <li>LangChain API</li> </ul>"},{"location":"learn/2025/07-july/2025-07-25/","title":"July 25, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-25/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-25/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-25/#leetcode-problem","title":"Leetcode Problem","text":"<p>1004. Max Consecutive Ones III</p>"},{"location":"learn/2025/07-july/2025-07-25/#thinking-process","title":"Thinking Process","text":"<p>The key to solving this problem is recognizing that finding the longest sequence of consecutive ones by flipping at most k zeros is equivalent to computing, for each index, the length of the longest valid sequence of ones ending there, and then taking the maximum of those lengths. We can use the sliding window technique to keep track of the current longest sequence. For every new element introduced right after the current window, if it is 1, we just extend the window. Otherwise, we flip it. If we use more flips than we needed, we need to reduce the window until it is valid again.</p>"},{"location":"learn/2025/07-july/2025-07-25/#complexity-analysis","title":"Complexity Analysis","text":"<ul> <li>Time Complexity: O(n)</li> <li>Space Complexity: O(1)</li> </ul>"},{"location":"learn/2025/07-july/2025-07-25/#langchain","title":"LangChain","text":""},{"location":"learn/2025/07-july/2025-07-25/#chat-models","title":"Chat Models","text":"<p>One thing we must realize before we dive deep into the ChatModel APIs is the additional capabilities that the modern LLM have. In the past, the only functionality of LLM is to take in a plain string and predict what comes after that string. Now it has some extra features like: tool calling, structured output and multimodality. Notice that these are built-in features of the models which means you can directly call model APIs to achieve all of them. What LangChain does here is that it incorporates all of them and provided a universal API for sake of developers. </p> <p>outdated model types</p> <p>As mentioned above, the modern LLM has been built with extra capabilities. These models implement <code>BaseChatModel</code> interface. Usually they have a <code>Chat</code> prefix in their variable name like <code>ChatOpenAI</code>. For these older models on the other hand, they have a \"LLM\" suffix like <code>OllamaLLM</code>. They just take in string and output string. Generally, just don't use these. </p> <p>Models have these key methods:</p> <ul> <li>invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.</li> <li>stream: A method that allows you to stream the output of a chat model as it is generated.</li> <li>batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.</li> <li>bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.</li> <li>with_structured_output: A wrapper around the invoke method for models that natively support structured output.</li> </ul> <p>They takes in messages and return messages as output.</p> <p>OpenAI message format</p> <p>OpenAI uses messages-based format for model interaction. Instead of just a single string, messages are a list of dictionaries where each dictionary has a <code>role</code> and <code>content</code> key. The role represent where the message comes from. If its value is <code>system</code> it refers to the fact that this message is for setting up the initial persona of the model or guidelines. <code>user</code> means user input query. <code>assistant</code> refers to a reply of the model and <code>tool</code> indicates that the message contains the output from a tool or function that is required by the model. </p> <p>You can set the temperature, model type, api url, rate limit related parameters in the chat model object.</p>"},{"location":"learn/2025/07-july/2025-07-25/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>PurpCorn-PLAN </li> <li>Next.js 16</li> </ul>"},{"location":"learn/2025/07-july/2025-07-26/","title":"July 26, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-26/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-26/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-26/#leetcode-problem","title":"Leetcode Problem","text":"<p>40. Combination Sum II</p>"},{"location":"learn/2025/07-july/2025-07-26/#thinking-process","title":"Thinking Process","text":"<p>The moment we saw combinations and permutations we can start to think of backtracking. There are mainly two subproblems we need to figure out. The first one is when to prune the state-space tree. The other is how to avoid duplication in the answer. The first question is actually quite easy. As we are doing backtracking, if the current combination is already larger than the target, we should prune it. The second one is a bit more tricky. The duplication would only happen if there are duplicate candidates. For example, <code>candidates = [10,1,2,7,6,1,5], target = 8</code>. The answer <code>[1, 7], [7,1]</code> would be duplicates in the return result. A good way to eliminate duplication is by sorting. Here if we sort the candidates before the backtracking, the returned answer would also be in sort. Now, we can think of when a duplication would happen. Should we avoid all duplicate candidates? Of course not, in the previous example, <code>[1, 1, 6]</code> would be a valid answer. The key is avoid duplication that happens on the same level of the state-space tree. <code>[1, 1, 6]</code> is not a problem since the first <code>1</code> has the depth of <code>1</code> in state-space tree. While <code>[1, 7]</code> could cause duplication since after we travese through all possible combination that starts with <code>[1, ...]</code> and backtrack, we hit another <code>1</code>. As a result, we need to check if the next candidate on the same level is equal to the previous candidate. </p>"},{"location":"learn/2025/07-july/2025-07-26/#implementation","title":"Implementation","text":"<p>class Solution:     def combinationSum2(self, candidates: List[int], target: int) -&gt; List[List[int]]:         ret = []         candidates.sort()         def backtrack(current, cur_sum, cur_idx):             if cur_sum &gt; target:                 return             if cur_sum == target:                 tuple(current)                 ret.append(current[:])                 return             for i in range(cur_idx, len(candidates)):                 if i &gt; cur_idx and candidates[i] == candidates[i - 1]:                     continue                 current.append(candidates[i])                 backtrack(current, cur_sum + candidates[i], i + 1)                 current.pop()             return         backtrack([], 0, 0)         return ret</p>"},{"location":"learn/2025/07-july/2025-07-26/#complexity-analysis","title":"Complexity Analysis","text":"<p>Assume there is <code>n</code> candidates in the array</p> <ul> <li>Time Complexity: O(2<sup>n</sup>)</li> <li>Space Complexity: O(n)</li> </ul> <p>In the worst case, let's say the target is greater than the sum of all possible combinations. We still need to traverse through all possible ones. For space, we are keep track of the current list to store the possilbe combinations. The worst case it needs O(n) space.</p>"},{"location":"learn/2025/07-july/2025-07-26/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Use sorting to avoid duplication</li> <li>Think in terms of state-space tree would help you understand the promblem better</li> <li>Get familiar with the backtrack pattern.</li> </ul> <p>152. Maximum Product Subarray</p>"},{"location":"learn/2025/07-july/2025-07-26/#thinking-process_1","title":"Thinking Process","text":"<p>A maximum product subarray must end with certain index. So if we know the maximum producet subarary ending with each index respectively, we can get the answer. The trickest part of the problem is to deal with negative numbers and zero. Say you got a negative number right now and you know the maximum product of the subarray ending in the previous index. Can you tell the maximum product including the current negative number? Actually no. To get a maximum product of containing the negative, you might want the minimum product of the previous subarray. Here is the pseudo code to do that:</p> <pre><code>temp_max = max(curr, max(max_so_far * curr, min_so_far * curr))\nmin_so_far = min(curr, min(max_so_far * curr, min_so_far * curr))\nmax_so_far = temp_max\n</code></pre> <p>This code actually takes care of all possible situation for both positive, negative and 0 all at once. </p>"},{"location":"learn/2025/07-july/2025-07-26/#complexity-analysis_1","title":"Complexity Analysis","text":"<p>Assume there are <code>n</code> numebrs in the <code>nums</code> array</p> <ul> <li>Time Complexity: O(n)</li> <li>Space Complexity: O(1)</li> </ul>"},{"location":"learn/2025/07-july/2025-07-26/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>Saas AI Governance</li> </ul>"},{"location":"learn/2025/07-july/2025-07-27/","title":"July 27, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-27/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-27/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-27/#trie","title":"Trie","text":""},{"location":"learn/2025/07-july/2025-07-27/#definition","title":"Definition","text":"<p>Trie is a special form a Nary tree that stores strings. Each node represents a prefix. And each edges represent a character that helps you transit between prefix node to another. </p> <p>From the above definition we can tell that any nodes that share the same parent has the same prefix.</p> <p>how a string is stored in trie</p> <p>Notice the string is not stored in any of the nodes. Instead it is stored as a path to certain nodes.</p>"},{"location":"learn/2025/07-july/2025-07-27/#implement-trie-node","title":"Implement Trie Node","text":"<pre><code>class TrieNode:\n\n    def __init__(self):\n        self.links = [None] * 26\n        self.is_word = False\n\n    def get_idx(self, char):\n        return ord(char.lower()) - ord(\"a\")\n\n    def get(self, char):\n        return self.links[self.get_idx(char)]\n\n    def put(self, char):\n        self.links[self.get_idx(char)] = TrieNode()\n\n    def has(self,char):\n        return self.links[self.get_idx(char)] is not None\n\n    def is_word(self):\n        return self.is_word\n\n    def set_word(self):\n        self.is_word = True\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-27/#implement-trie","title":"Implement Trie","text":"<pre><code>class Trie:\n\n    def __init__(self):\n        self.root = TrieNode()\n\n    def insert(self, word):\n        cur = self.root\n        for char in word:\n            if cur.has(char):\n                cur = cur.get(char)\n            else:\n                cur.put(char)\n                cur = cur.get(char)\n        cur.set_end()\n\n    def _search_prefix(self, prefix):\n        cur = self.root\n        for char in prefix:\n            if cur.has(char):\n                cur = cur.get(char)\n            else:\n                return None\n        return cur\n\n    def search(self, word):\n        node = self._search_prefiex(word)\n        return node and node.is_end()\n\n    def starts_with(self, prefix):\n        return self._search_prefix(word) is not None\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-27/#key-takeaway","title":"Key Takeaway","text":"<ul> <li>To implement the neighbors or links of <code>TrieNode</code> we can actually use a list with 26 elements to represent <code>a - z</code>. Then we can use <code>ord(char) - ord('a')</code> to quickly route the path. </li> <li>Notice, we need to have a <code>is_word</code> attribute in the <code>TrieNode</code> since it does not distinguish between prefixes and an actual word after insertion.</li> </ul>"},{"location":"learn/2025/07-july/2025-07-27/#3597-partition-string","title":"3597. Partition String","text":""},{"location":"learn/2025/07-july/2025-07-27/#thinking-process","title":"Thinking Process","text":"<p>This question is essentially a practice for Trie. We can iterate through each character and during each iteration, we keep track of what is the current segment is and where we are in the trie. For a new candidate, we check if hte trie has a link to an existing node that represnt the new character. If not we append the current segment to the result and reset the current segement and the node.</p>"},{"location":"learn/2025/07-july/2025-07-27/#implementation","title":"Implementation","text":"<pre><code>def get_idx(char):\n    return ord(char) - ord(\"a\")\n\nclass TrieNode:\n    def __init__(self):\n        self.links = [None] * 26\n\n    def has(self, char):\n        return self.links[get_idx(char)] is not None\n\n    def get(self, char):\n        return self.links[get_idx(char)]\n\n    def put(self, char):\n        self.links[get_idx(char)] = TrieNode()\n\nclass Solution:\n    def partitionString(self, s: str) -&gt; List[str]:\n        root = TrieNode()\n        node = root\n        ret = []\n        cur_seg = []\n        for c in s:\n            cur_seg.append(c)\n            if node.has(cur_seg[-1]):\n                node = node.get(cur_seg[-1])\n            else:\n                node.put(cur_seg[-1])\n                ret.append(\"\".join(cur_seg))\n                node = root\n                cur_seg = []\n        return ret\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-27/#complexity-analysis","title":"Complexity Analysis","text":"<p>Let the lenght of the string be <code>n</code></p> <ul> <li>Time Complexity: O(n)</li> <li>Space Complexity: O(n)</li> </ul> <p>We iterate through each character and performed some constant time action to it. Only non constant action would be the <code>\"\".join()</code> However, if we think in terms of each character, we would realize each character only contributed once to the loop of the join method. Thus it is still O(n) time. For space, since we are maintaining a trie. Each character would be stored in the worst case. And we are keep a current segment list as well.</p>"},{"location":"learn/2025/07-july/2025-07-27/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>Pillar AI security</li> </ul>"},{"location":"learn/2025/07-july/2025-07-28/","title":"July 28, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-28/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-28/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-28/#898-bitwise-ors-of-subarrays","title":"898. Bitwise ORs of Subarrays","text":""},{"location":"learn/2025/07-july/2025-07-28/#thinking-process","title":"Thinking Process","text":"<p>It is easy to come up with a naive solution where we just check all subarrays and its bitwise or result and put them in a set. This would take O(n<sup>2</sup>) time. Since the input size is 5 * 10e4 this is not viable. Let's think incrementally. What if we know the solution of unique results of the previous arrays? With a new element coming in, we are only interested in the subarrays that ends with the new element. Denote the new element's index as <code>k</code>. To calculate the distinct bit wise ors of subarray ending in <code>k</code>, the best way is to keep track of the results of previous subarrays that ends in <code>k - 1</code>. Then we can just <code>|</code> with each of them and append <code>k</code> to it. This seems to be a linear time operation for each of the new element. However, if you take a closer look at the prompt you, would find that the <code>arr[i]</code> is bounded by <code>[0, 10^9^]</code> which has at most 32 bits. The bitwise ors we keep track of is incremental, thus we can have at most 32 elements in the frontier set. </p>"},{"location":"learn/2025/07-july/2025-07-28/#implementation","title":"Implementation","text":"<pre><code>def bitwiseOperation(nums):\n    frontier = set({0})\n    result = set()\n    for num in nums:\n        frontier = {num | x for x in frontier} | {num}\n        result |= frontier\n    return len(result)\n</code></pre>"},{"location":"learn/2025/07-july/2025-07-28/#complexity-analysis","title":"Complexity Analysis","text":"<p>Let the longest bit length of the element be <code>m</code>. And assume there is <code>n</code> elements in the array</p> <ul> <li>Time Complexity: O(nlog(m))</li> <li>Space Complexity: O(nm)</li> </ul> <p>As we mentioned above, we would insert each element into set for <code>n</code> times. Each operation would take <code>log(m)</code> time. Thus O(nlog(m)) for time complexity. For space, assume for the worst case, each frontier set is different. The there would be n * m elements in stored in the result</p>"},{"location":"learn/2025/07-july/2025-07-28/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>Gemini 2 Deepthinking </li> </ul>"},{"location":"learn/2025/07-july/2025-07-29/","title":"July 29, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-29/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-29/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-29/#ryos","title":"ryos","text":""},{"location":"learn/2025/07-july/2025-07-29/#reflection","title":"Reflection","text":"<p>This is an interesting web project built by Ryo Lu, Cursor's Head of Design. He designed a web page to mimic old macintosh os. It has a web broswer that allows you to see different websites in different times even in the future. The future ones are AI generated in real time with careful prompt template and style specification. There are all sorts of other \"softwares\" available on the website that allows you to play with. Like a video camera with  weird shaders, a video player that stores all ryo's favorite old clips, and a regular minesweeper. A lots of the code is actually written using AI. Two things that Ryo said left a particularly deep impression on me. One is how important it is to just play around with the technology to understand it fully. No matter it is learning web development or AI or vide coding. Just start with a project that interests you and get your hands dirty. Another things is about vibe coding. With AI, we can actually implement so many our ideas just with a few prompts. Some people just think of vibe coding as putting in their crazy idea in the chat window and expects it to work perfectly. It usually doesn't work like that. You need to have the ability to decompose bigger promblems into smaller ones. Understand how your architecture is gonna belike. Provide the correct context and prompt to the AI and then you might get what you want at the first place. </p>"},{"location":"learn/2025/07-july/2025-07-29/#211-design-add-and-search-words-data-structure","title":"211. Design Add and Search Words Data Structure","text":""},{"location":"learn/2025/07-july/2025-07-29/#thinking-process","title":"Thinking Process","text":"<p>At first glance, I think of a brute force solution where we just add every word into a set and during the search we just generate all possible strings and check if that is valid. The brute force is a bit tricky since we need to flatten the nested for loop to generate all possible combinations. My implementation is to first check for how many wild card there is. And loop from 0 to 2<sup>k</sup> - 1 where k equals to the number of wildcard symbol. Then we map the integer to list of character. The way to think of the number is to convert it into a base-26 number. Then we can map each digit into a character. Anyways, the better way is to use Trie. That way we can prune the search tree early. Since if we don't have certain prefix of the string, we can call for a stop immediately.</p>"},{"location":"learn/2025/07-july/2025-07-29/#complexity-analysis","title":"Complexity Analysis","text":"<p>Assume we have <code>n</code> elements already in the set. And the word for insertion and search has length <code>l</code></p> <p>Brutal Force </p> <ul> <li>Time Complexity: O(log(n)) for add word; O(26<sup>l</sup>log(n)) for word search</li> <li>Space Complexity: O(n)</li> </ul> <p>Trie</p> <ul> <li>Time Complexity: O(l) for add word; O(26<sup>l</sup>) for word search</li> <li>Space Complexity: O(n)</li> </ul> <p>For word search, it would usually end within O(l). In the worst case, we need to search for all the nodes. Say we got a input string like \"..z\" , and we have all permuatations of the three-character word except for the ones end with z. Then we need to check every single node in the tree. Thus, O(26<sup>l</sup>)</p>"},{"location":"learn/2025/07-july/2025-07-29/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Use trie for word search can help with search tree pruning</li> <li>Use mapping to flatten nested for loops </li> </ul>"},{"location":"learn/2025/07-july/2025-07-29/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>awesome cursor rules</li> <li>jwt</li> <li>leetcode company wise</li> </ul>"},{"location":"learn/2025/07-july/2025-07-30/","title":"July 30, 2025 - Daily Drill","text":""},{"location":"learn/2025/07-july/2025-07-30/#daily-goals","title":"\ud83c\udfaf Daily Goals","text":"<ul> <li> Review Anki Deck</li> <li> Lumosity training</li> <li> Leetcode</li> <li> Developing laplace</li> <li> Langchain Learning</li> </ul>"},{"location":"learn/2025/07-july/2025-07-30/#what-i-learned","title":"\ud83d\udcdd What I learned:","text":""},{"location":"learn/2025/07-july/2025-07-30/#416-partition-equal-subset-sum","title":"416. Partition Equal Subset Sum","text":""},{"location":"learn/2025/07-july/2025-07-30/#thinking-process","title":"Thinking Process","text":"<p>This is a classic dynamic programming problem. The brutal force solution requires you to iterate through all possible subsets and cacluate their sum. This takes O(2<sup>n</sup>) time. Instead we can think from another perspective: what we can form using the elements from the array. If we only look at one element <code>k</code>, the answer would be <code>0</code> and <code>k</code>. And we can then introduce another new element <code>m</code>, The subset sum we can form becomes <code>0</code>, <code>k</code>, <code>m</code>, <code>k + m</code>. We can see that from this process, we can build a possible subset sum array within just O(n<sup>2</sup>) time. Then we can easily check if the <code>sum_of_all_element // 2</code> exists in the array. Using bit operation we can further improve the time complexity. The idea is as follows. If a sum <code>s</code> can be formed using all exists elements, we create a variable <code>bit_set</code> and mark <code>s</code>th bit to 1. When a new number <code>n</code> is introduced, we  can simply execute the operation <code>bit_set |= bit_set &lt;&lt; n</code>. To check if a sum is possible we can do <code>bit_set &gt;&gt; s &amp; 1 == 1</code>. </p>"},{"location":"learn/2025/07-july/2025-07-30/#complexity-analysis","title":"Complexity Analysis","text":"<p>Given that we have <code>n</code> elements in the array.</p> <ul> <li>Time Complexity: O(n)</li> <li>Space Complexity: O(1)</li> </ul>"},{"location":"learn/2025/07-july/2025-07-30/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Use bit operation to reduce time complexity</li> </ul>"},{"location":"learn/2025/07-july/2025-07-30/#resources-that-requires-further-study","title":"\ud83d\ude80 Resources that Requires Further Study","text":"<ul> <li>AI Coding Productivity</li> </ul> <p>This is an interesting research on the AI coding productivity. 16 experienced open-source developer (each with 5 or more years of experience) are selected to complete a series of tasks with/without the helper of AI code editor. The productivity increase with AI coder is predicted to be around 24%. Nonetheless, the result is quite surprising. The group of developer with AI coder actually get their productivity decreased by 19%. This might be caused by the facts that AI is not quite good at handling complex codebase and prompt and context engineering are also time consuming.</p>"},{"location":"resources/","title":"Resource Hub \ud83c\udfe6","text":"<p>Welcome to my resources! Here I store all kinds of resources in the fields I am interested in.</p>"},{"location":"resources/#topics","title":"\ud83c\udfaf Topics","text":"<ul> <li>Machine Learning &amp; AI - Deep learning, NLP, computer vision</li> <li>Web Development - React, Node.js, modern frameworks</li> <li>Systems &amp; Architecture - Distributed systems, cloud computing</li> <li>Research Projects - Academic papers and implementations</li> </ul> <p>\"The capacity to learn is a gift; the ability to learn is a skill; the willingness to learn is a choice.\" - Brian Herbert</p>"}]}