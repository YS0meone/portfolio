# July 17, 2025 - AI Engineer RAG

## üéØ Daily Goals

- [x] Review Anki Deck
- [x] Lumosity training
- [x] Finished AI Engineering RAG Chapter

## üìù What I learned:

### AI Engineer Book

#### The comparison between term-based retrieval and embedding-based retrieval

![comparison](https://res.cloudinary.com/dzwxmzyiy/image/upload/v1752788779/E33CB07E-90AA-4711-9778-1E73AF8178A2_bmgw8s.png)

#### Metrics for evaluation

**For retrieved documents only**
- Context Precision: The percentage of relevant documents among all retrieved ones
- Context Recall: The percentage of retrieved documents among all relevant ones

**Document ranking**

This expects the most important documents should be retrieved first

 - [NDCG (normalized discounted cumulative gain)](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
 - [MAP (Mean Average Precision)](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)
 - [MRR (Mean Reciprocal Rank)](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision)

**Embedding**

- [MTEB](https://arxiv.org/abs/2210.07316)

We should also evaluate the retriever with the model as a whole. The evaluation would be talked about in the future chapters

!!! note The most time consuming part of RAG system

    The most time consuming part of a RAG system is actually the output generation. The embedding generation and vector search is actually minimal when compared to it.

#### Design choice of retrieval system

Mainly, we consider two parts, *indexing cost* and *querying quality*.

To improve *query quality*, we use more detailed index which takes much more time and memory to create and thus getting higher *query quality*. One example of index of this kind is: [HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world)

To reduce *index cost*, we can use simpler index like [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing). This is easier to build and yet results in slower and less accurate queries.

We also have control over the *ANN algorithm* used during retrieval. Check the performance for each of them [here](https://ann-benchmarks.com/index.html).

#### Three aspects of RAG evaluation:

- Retrieval Quality 
  - Context precision, recall, query speed and accuracy, indexing efficiency
- Embedding (For embedding-based retrieval only)
  - MTEB
- RAG output
  - Evaluate LLM output

## üöÄ Resources that Requires Further Study:

- [Shipping Fast](https://www.intercom.com/blog/ship-fast-safe-learn-from-production/)