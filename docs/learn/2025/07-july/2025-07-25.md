# July 25, 2025 - Daily Drill

## üéØ Daily Goals

- [x] Review Anki Deck
- [x] Lumosity training
- [x] Leetcode
- [x] Developing laplace
- [x] Langchain Learning

## üìù What I learned:

### Leetcode Problem

[**1004. Max Consecutive Ones III**](https://leetcode.com/problems/max-consecutive-ones-iii/description/?envType=company&envId=amazon&favoriteSlug=amazon-thirty-days)

#### Thinking Process

The key to solving this problem is recognizing that finding the longest sequence of consecutive ones by flipping at most k zeros is equivalent to computing, for each index, the length of the longest valid sequence of ones ending there, and then taking the maximum of those lengths. We can use the sliding window technique to keep track of the current longest sequence. For every new element introduced right after the current window, if it is 1, we just extend the window. Otherwise, we flip it. If we use more flips than we needed, we need to reduce the window until it is valid again.

#### Complexity Analysis

- **Time Complexity: O(n)**
- **Space Complexity: O(1)**

### LangChain 

#### Chat Models

One thing we must realize before we dive deep into the ChatModel APIs is the additional capabilities that the modern LLM have. In the past, the only functionality of LLM is to take in a plain string and predict what comes after that string. Now it has some extra features like: tool calling, structured output and multimodality. Notice that these are built-in features of the models which means you can directly call model APIs to achieve all of them. What LangChain does here is that it incorporates all of them and provided a universal API for sake of developers. 

!!! note "outdated model types"

    As mentioned above, the modern LLM has been built with extra capabilities. These models implement `BaseChatModel` interface. Usually they have a `Chat` prefix in their variable name like `ChatOpenAI`. For these older models on the other hand, they have a "LLM" suffix like `OllamaLLM`. They just take in string and output string. Generally, just don't use these. 

Models have these key methods:

- invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.
- stream: A method that allows you to stream the output of a chat model as it is generated.
- batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.
- bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.
- with_structured_output: A wrapper around the invoke method for models that natively support structured output.

They takes in messages and return messages as output.

!!! info "OpenAI message format"

    OpenAI uses messages-based format for model interaction. Instead of just a single string, messages are a list of dictionaries where each dictionary has a `role` and `content` key. The role represent where the message comes from. If its value is `system` it refers to the fact that this message is for setting up the initial persona of the model or guidelines. `user` means user input query. `assistant` refers to a reply of the model and `tool` indicates that the message contains the output from a tool or function that is required by the model. 

You can set the temperature, model type, api url, rate limit related parameters in the chat model object.


## üöÄ Resources that Requires Further Study

- [PurpCorn-PLAN](https://www.amazon.science/nova-ai-challenge/pushing-the-boundaries-of-secure-ai-winners-of-the-amazon-nova-ai-challenge) 
- Next.js 16
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; margin: 1.5rem 0;">
    <iframe src="https://www.youtube.com/embed/eti5c_IeJgI" 
            style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;" 
            allowfullscreen>
    </iframe>
</div>

